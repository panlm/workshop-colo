{"api_version":"3.0","metadata":{"creation_time":"1545660572907394","kind":"blueprint","last_update_time":"1545721971571669","name":"COLO_Kubernetes_Demo_BP","spec_version":6},"spec":{"description":"* [Kubernetes API](https:\/\/@@{Kubernetes_Master.address[0]}@@:6443)\n* admin_certificate: @@{Kubernetes_Master.admin_certificate[0]}@@\n* admin_key: @@{Kubernetes_Master.admin_key[0]}@@","name":"COLO_Kubernetes_Demo_BP","resources":{"app_profile_list":[{"action_list":[{"critical":false,"description":"","name":"ScaleIn","runbook":{"description":"","main_task_local_reference":{"kind":"app_task","name":"e72a9133_dag"},"name":"96546f6c_runbook","task_definition_list":[{"attrs":{"scaling_count":"@@{COUNT}@@","scaling_type":"SCALEIN","type":""},"child_tasks_local_reference_list":[],"description":"","name":"Scale In","retries":"0","target_any_local_reference":{"kind":"app_blueprint_deployment","name":"39cf7283_deployment"},"timeout_secs":"0","type":"SCALING","variable_list":[]},{"attrs":{"edges":[],"type":""},"child_tasks_local_reference_list":[{"kind":"app_task","name":"Scale In"}],"description":"","name":"e72a9133_dag","retries":"0","timeout_secs":"0","type":"DAG","variable_list":[]}],"variable_list":[{"attrs":{"type":""},"description":"","editables":{"value":true},"label":"","name":"COUNT","type":"LOCAL","val_type":"STRING","value":"1"}]},"type":"user"},{"critical":false,"description":"","name":"ScaleOut","runbook":{"description":"","main_task_local_reference":{"kind":"app_task","name":"03b33121_dag"},"name":"00cf5417_runbook","task_definition_list":[{"attrs":{"edges":[{"edge_type":"user_defined","from_task_reference":{"kind":"app_task","name":"Scale Out"},"to_task_reference":{"kind":"app_task","name":"Set Hosts file"},"type":""}],"type":""},"child_tasks_local_reference_list":[{"kind":"app_task","name":"Scale Out"},{"kind":"app_task","name":"Set Hosts file"}],"description":"","name":"03b33121_dag","retries":"0","timeout_secs":"0","type":"DAG","variable_list":[]},{"attrs":{"scaling_count":"@@{COUNT}@@","scaling_type":"SCALEOUT","type":""},"child_tasks_local_reference_list":[],"description":"","name":"Scale Out","retries":"0","target_any_local_reference":{"kind":"app_blueprint_deployment","name":"39cf7283_deployment"},"timeout_secs":"0","type":"SCALING","variable_list":[]},{"attrs":{"command_line_args":"","exit_status":[],"script":"#!\/bin\/bash\nset -ex\n\nMINION_IPS=\"@@{AHV_Centos_K8SM.address}@@\"\nfor ip in $(echo ${MINION_IPS} | tr \",\" \"\\n\"); do\n  if ! (( $(grep -c \"${ip} minion${count}\" \/etc\/hosts) )) ; then\n  \techo \"${ip} minion${count}\" | sudo -E tee -a \/etc\/hosts\n  fi\n  count=$((count+1))\ndone","script_type":"sh","type":""},"child_tasks_local_reference_list":[],"description":"","name":"Set Hosts file","retries":"0","target_any_local_reference":{"kind":"app_service","name":"Kubernetes_Master"},"timeout_secs":"0","type":"EXEC","variable_list":[]}],"variable_list":[{"attrs":{"type":""},"description":"","editables":{"value":true},"label":"","name":"COUNT","type":"LOCAL","val_type":"STRING","value":"1"},{"attrs":{"type":""},"description":"","editables":{"value":true},"label":"","name":"KUBE_IMAGE_TAG_NEW","type":"LOCAL","val_type":"STRING","value":"v1.10.5_coreos.0"}]},"type":"user"},{"critical":false,"description":"","name":"Upgrade","runbook":{"description":"","main_task_local_reference":{"kind":"app_task","name":"6ee65d8a_dag"},"name":"68a7ffd0_runbook","task_definition_list":[{"attrs":{"edges":[{"edge_type":"user_defined","from_task_reference":{"kind":"app_task","name":"Upgrade Controller"},"to_task_reference":{"kind":"app_task","name":"Restart Service"},"type":""},{"edge_type":"user_defined","from_task_reference":{"kind":"app_task","name":"Upgrade Minion"},"to_task_reference":{"kind":"app_task","name":"Restart Service"},"type":""}],"type":""},"child_tasks_local_reference_list":[{"kind":"app_task","name":"Restart Service"},{"kind":"app_task","name":"Upgrade Controller"},{"kind":"app_task","name":"Upgrade Minion"}],"description":"","name":"6ee65d8a_dag","retries":"0","timeout_secs":"0","type":"DAG","variable_list":[]},{"attrs":{"command_line_args":"","exit_status":[],"script":"#!\/bin\/bash\nset -ex\n\nif [ @@{calm_array_index}@@ -ne 0 ];then\n\texit\nfi\n\nINTERNAL_IP=\"@@{address}@@\"\nCONTROLLER_IPS=\"@@{calm_array_address}@@\"\nMINION_IPS=\"@@{AHV_Centos_K8SM.address}@@\"\nJSONPATH='{range .items[*]}{@.metadata.name}:{range @.status.conditions[*]}{@.type}={@.status};{end}{end}'\nKUBELET_IMAGE_TAG=\"@@{KUBE_IMAGE_TAG_NEW}@@\"\nMASTER_API_HTTPS=6443\nVERSION=$(echo \"${KUBELET_IMAGE_TAG}\" | tr \"_\" \" \" | awk '{print $1}')\nwget -q https:\/\/storage.googleapis.com\/kubernetes-release\/release\/${VERSION}\/bin\/linux\/amd64\/kubectl\nchmod +x kubectl\nsudo -E mv kubectl \/usr\/local\/bin\/kubectl\n\ncount=0\nwhile [[ $(curl --key CA\/admin-key.pem --cert CA\/admin.pem --cacert CA\/ca.pem https:\/\/${INTERNAL_IP}:${MASTER_API_HTTPS}\/healthz) != \"ok\" ]] ; do\n  echo \"Trying to reach master server https:\/\/${INTERNAL_IP}:${MASTER_API_HTTPS} : sleep for 5 secs\"\n  sleep 10\n  if [[ $count -eq 10 ]]; then\n  \techo \"Unable to reach master server: https:\/\/${INTERNAL_IP}:${MASTER_API_HTTPS}\"\n  \texit 1\n  fi\n  count=$((count+1))\ndone\n\ncount=0\nfor ip in $(echo \"${MINION_IPS}\" | tr \",\" \"\\n\"); do\n  kubectl drain \"minion${count}\" --ignore-daemonsets --delete-local-data --force\n  sleep 20\n  ssh -o stricthostkeychecking=no $ip \"sudo -E yum update -y --quiet && sudo -E systemctl daemon-reload && sudo -E systemctl restart kubelet\"\n  sleep 20\n  while [[ `kubectl get nodes -l kubernetes.io\/hostname=minion${count} -o jsonpath=\"$JSONPATH\" | grep \"Ready=Unknown\" 2>\/dev\/null` ]]; do sleep 10 ; done\n  kubectl uncordon minion${count}\n  count=$((count+1))\ndone\n\n\nif [[ `kubectl get nodes -o jsonpath=\"$JSONPATH\" | grep \"Ready=Unknown\"` ]]; then \n\techo \"Upgrade failed on minion nodes\"\n    exit 1\nfi\n\ncount=0\nfor ip in $(echo \"${CONTROLLER_IPS}\" | tr \",\" \"\\n\"); do\n  kubectl drain \"controller${count}\" --ignore-daemonsets --delete-local-data --force\n  sleep 20\n  ssh -o stricthostkeychecking=no $ip \"sudo -E yum update -y --quiet && sudo -E systemctl daemon-reload && sudo -E systemctl restart kubelet\"\n  sleep 20\n  while [[ `kubectl get nodes -l kubernetes.io\/hostname=controller${count} -o jsonpath=\"$JSONPATH\" | grep \"Ready=Unknown\" 2>\/dev\/null` ]]; do sleep 10 ; done\n  kubectl uncordon controller${count}\n  count=$((count+1))\ndone\n\n\nif [[ `kubectl get nodes -o jsonpath=\"$JSONPATH\" | grep \"Ready=Unknown\"` ]]; then \n\techo \"Upgrade failed on nodes: $(kubectl get nodes -o jsonpath='$JSONPATH' | grep 'Ready=Unknown')\"\n    exit 1\nfi","script_type":"sh","type":""},"child_tasks_local_reference_list":[],"description":"","name":"Restart Service","retries":"0","target_any_local_reference":{"kind":"app_service","name":"Kubernetes_Master"},"timeout_secs":"0","type":"EXEC","variable_list":[]},{"attrs":{"command_line_args":"","exit_status":[],"script":"#!\/bin\/bash\nset -ex\n\nETCD_VERSION=\"v3.2.11\"\nINTERNAL_IP=\"@@{address}@@\"\nCONTROLLER_IPS=\"@@{calm_array_address}@@\"\nNODE_NAME=\"controller@@{calm_array_index}@@\"\nCLUSTER_SUBNET=\"@@{KUBE_CLUSTER_SUBNET}@@\"\nSERVICE_SUBNET=\"@@{KUBE_SERVICE_SUBNET}@@\"\nKUBE_CLUSTER_DNS=\"@@{KUBE_DNS_IP}@@\"\nKUBELET_IMAGE_TAG=\"@@{KUBE_IMAGE_TAG_NEW}@@\"\nDOCKER_VERSION=\"@@{DOCKER_VERSION}@@\"\nETCD_CERT_PATH=\"\/etc\/ssl\/certs\/etcd\"\nKUBE_CERT_PATH=\"\/etc\/kubernetes\/ssl\"\nKUBE_MANIFEST_PATH=\"\/etc\/kubernetes\/manifests\"\nVERSION=$(echo \"${KUBELET_IMAGE_TAG}\" | tr \"_\" \" \" | awk '{print $1}')\nMASTER_API_HTTPS=6443\nETCD_SERVER_PORT=2379\nCONTROLLER_COUNT=$(echo \"@@{calm_array_address}@@\" | tr ',' '\\n' | wc -l)\n\n\ncount=0\nfor ip in $(echo \"${CONTROLLER_IPS}\" | tr \",\" \"\\n\"); do\n  ETCD+=\"https:\/\/${ip}:${ETCD_SERVER_PORT}\",\n  count=$((count+1))\ndone\nETCD_SERVERS=$(echo $ETCD | sed  's\/,$\/\/')\n\nwget -q https:\/\/storage.googleapis.com\/kubernetes-release\/release\/${VERSION}\/bin\/linux\/amd64\/kubelet\nchmod +x kubelet\nsudo -E mv kubelet \/usr\/bin\/kubelet\n\necho \"[Unit]\nDescription=Kubernetes Kubelet\nDocumentation=https:\/\/github.com\/GoogleCloudPlatform\/kubernetes\nAfter=docker.service\nRequires=docker.service\n\n[Service]\nExecStart=\/usr\/bin\/kubelet \\\\\n  --allow-privileged=true \\\\\n  --anonymous-auth=false \\\\\n  --authorization-mode=Webhook \\\\\n  --cluster-dns=${KUBE_CLUSTER_DNS} \\\\\n  --cluster-domain=cluster.local \\\\\n  --container-runtime=docker \\\\\n  --enable-custom-metrics \\\\\n  --kubeconfig=${KUBE_CERT_PATH}\/${NODE_NAME}.kubeconfig \\\\\n  --network-plugin=cni \\\\\n  --pod-cidr=${CLUSTER_SUBNET} \\\\\n  --register-node=true \\\\\n  --runtime-request-timeout=10m \\\\\n  --client-ca-file=${KUBE_CERT_PATH}\/ca.pem \\\\\n  --tls-cert-file=${KUBE_CERT_PATH}\/${NODE_NAME}.pem \\\\\n  --tls-private-key-file=${KUBE_CERT_PATH}\/${NODE_NAME}-key.pem \\\\\n  --pod-manifest-path=${KUBE_MANIFEST_PATH} \\\\\n  --read-only-port=0 \\\\\n  --protect-kernel-defaults=false \\\\\n  --make-iptables-util-chains=true \\\\\n  --keep-terminated-pod-volumes=false \\\\\n  --event-qps=0 \\\\\n  --cadvisor-port=0 \\\\\n  --runtime-cgroups=\/systemd\/system.slice \\\\\n  --kubelet-cgroups=\/systemd\/system.slice \\\\\n  --node-labels 'node-role.kubernetes.io\/master=true' \\\\\n  --node-labels 'node-role.kubernetes.io\/etcd=true' \\\\\n  --register-with-taints=node-role.kubernetes.io\/master=true:NoSchedule \\\\\n  --v=2\nRestart=on-failure\nRestartSec=5\n\n[Install]\nWantedBy=multi-user.target\" | sudo -E tee \/etc\/systemd\/system\/kubelet.service\n\necho \"apiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-apiserver\n  namespace: kube-system\n  labels:\n    k8s-app: kube-apiserver\nspec:\n  hostNetwork: true\n  containers:\n  - name: kube-apiserver\n    image: quay.io\/coreos\/hyperkube:${KUBELET_IMAGE_TAG}\n    command:\n    - \/hyperkube\n    - apiserver\n    - --admission-control=Initializers,NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota\n    - --advertise-address=${INTERNAL_IP}\n    - --allow-privileged=true\n    - --anonymous-auth=false\n    - --insecure-port=0\n    - --secure-port=${MASTER_API_HTTPS}\n    - --profiling=false\n    - --repair-malformed-updates=false\n    - --apiserver-count=${CONTROLLER_COUNT}\n    - --audit-log-maxage=30\n    - --audit-log-maxbackup=10\n    - --audit-log-maxsize=100\n    - --audit-log-path=\/var\/lib\/audit.log\n    - --authorization-mode=Node,RBAC\n    - --bind-address=0.0.0.0\n    - --kubelet-preferred-address-types=InternalIP,Hostname,ExternalIP\n    - --event-ttl=1h\n    - --service-account-lookup=true\n    - --enable-swagger-ui=true\n    - --storage-backend=etcd3\n    - --etcd-cafile=${ETCD_CERT_PATH}\/etcd-ca.pem\n    - --etcd-certfile=${ETCD_CERT_PATH}\/etcd-client.pem\n    - --etcd-keyfile=${ETCD_CERT_PATH}\/etcd-client-key.pem\n    - --etcd-servers=${ETCD_SERVERS}\n    - --experimental-encryption-provider-config=${KUBE_CERT_PATH}\/encryption-config.yaml\n    - --tls-ca-file=${KUBE_CERT_PATH}\/ca.pem\n    - --tls-cert-file=${KUBE_CERT_PATH}\/kubernetes.pem\n    - --tls-private-key-file=${KUBE_CERT_PATH}\/kubernetes-key.pem\n    - --kubelet-client-certificate=${KUBE_CERT_PATH}\/kubernetes.pem\n    - --kubelet-client-key=${KUBE_CERT_PATH}\/kubernetes-key.pem\n    - --kubelet-https=true\n    - --runtime-config=api\/all\n    - --service-account-key-file=${KUBE_CERT_PATH}\/kubernetes.pem\n    - --service-cluster-ip-range=${SERVICE_SUBNET}\n    - --service-node-port-range=30000-32767\n    - --client-ca-file=${KUBE_CERT_PATH}\/ca.pem\n    - --v=2\n    ports:\n    - containerPort: ${MASTER_API_HTTPS}\n      hostPort: ${MASTER_API_HTTPS}\n      name: https\n    - containerPort: 8080\n      hostPort: 8080\n      name: local\n    volumeMounts:\n    - mountPath: ${KUBE_CERT_PATH}\n      name: ssl-certs-kubernetes\n      readOnly: true\n    - mountPath: \/etc\/ssl\/certs\n      name: ssl-certs-host\n      readOnly: true\n    - mountPath: \/etc\/pki\n      name: ca-certs-etc-pki\n      readOnly: true\n  volumes:\n  - hostPath:\n      path: ${KUBE_CERT_PATH}\n    name: ssl-certs-kubernetes\n  - hostPath:\n      path: \/etc\/ssl\/certs\n    name: ssl-certs-host\n  - hostPath:\n      path: \/etc\/pki\n    name: ca-certs-etc-pki\" | sudo -E tee ${KUBE_MANIFEST_PATH}\/kube-apiserver.yaml\n\necho \"apiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-proxy\n  namespace: kube-system\n  labels:\n    k8s-app: kube-proxy\nspec:\n  hostNetwork: true\n  containers:\n  - name: kube-proxy\n    image: quay.io\/coreos\/hyperkube:${KUBELET_IMAGE_TAG}\n    command:\n    - \/hyperkube\n    - proxy\n    - --cluster-cidr=${CLUSTER_SUBNET}\n    - --masquerade-all=true\n    - --kubeconfig=${KUBE_CERT_PATH}\/kube-proxy.kubeconfig\n    - --proxy-mode=iptables\n    securityContext:\n      privileged: true\n    volumeMounts:\n    - mountPath: ${KUBE_CERT_PATH}\n      name: ssl-certs-kubernetes\n      readOnly: true\n    - mountPath: \/etc\/ssl\/certs\n      name: ssl-certs-host\n      readOnly: true\n  volumes:\n  - hostPath:\n      path: ${KUBE_CERT_PATH}\n    name: ssl-certs-kubernetes\n  - hostPath:\n      path: \/etc\/ssl\/certs\n    name: ssl-certs-host\" | sudo -E tee ${KUBE_MANIFEST_PATH}\/kube-proxy.yaml\n    \necho \"apiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-controller-manager\n  namespace: kube-system\n  labels:\n    k8s-app: kube-controller-manager\nspec:\n  hostNetwork: true\n  containers:\n  - name: kube-controller-manager\n    image: quay.io\/coreos\/hyperkube:${KUBELET_IMAGE_TAG}\n    command:\n    - \/hyperkube\n    - controller-manager\n    - --address=0.0.0.0  \n    - --allocate-node-cidrs=true  \n    - --cluster-cidr=${CLUSTER_SUBNET}\n    - --cluster-name=kubernetes-prod-cluster  \n    - --leader-elect=true  \n    - --kubeconfig=${KUBE_CERT_PATH}\/kube-controller-manager.kubeconfig  \n    - --service-account-private-key-file=${KUBE_CERT_PATH}\/kubernetes-key.pem  \n    - --service-cluster-ip-range=${SERVICE_SUBNET}\n    - --terminated-pod-gc-threshold=100  \n    - --profiling=false  \n    - --use-service-account-credentials=true  \n    - --v=2\n    livenessProbe:\n      httpGet:\n        host: 127.0.0.1\n        path: \/healthz\n        port: 10252\n      initialDelaySeconds: 15\n      timeoutSeconds: 1\n    volumeMounts:\n    - mountPath: ${KUBE_CERT_PATH}\n      name: ssl-certs-kubernetes\n      readOnly: true\n    - mountPath: \/etc\/ssl\/certs\n      name: ssl-certs-host\n      readOnly: true\n    - mountPath: \/etc\/pki\n      name: ca-certs-etc-pki\n      readOnly: true\n  volumes:\n  - hostPath:\n      path: ${KUBE_CERT_PATH}\n    name: ssl-certs-kubernetes\n  - hostPath:\n      path: \/etc\/ssl\/certs\n    name: ssl-certs-host\n  - hostPath:\n      path: \/etc\/pki\n    name: ca-certs-etc-pki\" | sudo -E tee ${KUBE_MANIFEST_PATH}\/kube-controller-manager.yaml\n    \necho \"apiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-scheduler\n  namespace: kube-system\n  labels:\n    k8s-app: kube-scheduler\nspec:\n  hostNetwork: true\n  containers:\n  - name: kube-scheduler\n    image: quay.io\/coreos\/hyperkube:${KUBELET_IMAGE_TAG}\n    command:\n    - \/hyperkube\n    - scheduler\n    - --kubeconfig=${KUBE_CERT_PATH}\/kube-scheduler.kubeconfig\n    - --leader-elect=true\n    - --profiling=false\n    - --v=2\n    livenessProbe:\n      httpGet:\n        host: 127.0.0.1\n        path: \/healthz\n        port: 10251\n      initialDelaySeconds: 15\n      timeoutSeconds: 1\n    volumeMounts:\n    - mountPath: ${KUBE_CERT_PATH}\n      name: ssl-certs-kubernetes\n      readOnly: true\n  volumes:\n  - hostPath:\n      path: ${KUBE_CERT_PATH}\n    name: ssl-certs-kubernetes\" | sudo -E tee ${KUBE_MANIFEST_PATH}\/kube-scheduler.yaml","script_type":"sh","type":""},"child_tasks_local_reference_list":[],"description":"","name":"Upgrade Controller","retries":"0","target_any_local_reference":{"kind":"app_service","name":"Kubernetes_Master"},"timeout_secs":"0","type":"EXEC","variable_list":[]},{"attrs":{"command_line_args":"","exit_status":[],"script":"#!\/bin\/bash\nset -ex\n\nINTERNAL_IP=\"@@{address}@@\"\nNODE_NAME=\"minion@@{calm_array_index}@@\"\nCLUSTER_SUBNET=\"@@{KUBE_CLUSTER_SUBNET}@@\"\nKUBE_CLUSTER_DNS=\"@@{KUBE_DNS_IP}@@\"\nKUBELET_IMAGE_TAG=\"@@{KUBE_IMAGE_TAG_NEW}@@\"\nDOCKER_VERSION=\"@@{DOCKER_VERSION}@@\"\nKUBE_CERT_PATH=\"\/etc\/kubernetes\/ssl\"\nKUBE_MANIFEST_PATH=\"\/etc\/kubernetes\/manifests\"\nVERSION=$(echo \"${KUBELET_IMAGE_TAG}\" | tr \"_\" \" \" | awk '{print $1}')\n\nwget -q https:\/\/storage.googleapis.com\/kubernetes-release\/release\/${VERSION}\/bin\/linux\/amd64\/kubelet\nchmod +x kubelet\nsudo -E mv kubelet \/usr\/bin\/kubelet\n\necho \"[Unit]\nDescription=Kubernetes Kubelet\nDocumentation=https:\/\/github.com\/GoogleCloudPlatform\/kubernetes\nAfter=docker.service\nRequires=docker.service\n\n[Service]\nExecStart=\/usr\/bin\/kubelet \\\\\n  --allow-privileged=true \\\\\n  --anonymous-auth=false \\\\\n  --authorization-mode=Webhook \\\\\n  --cluster-dns=${KUBE_CLUSTER_DNS} \\\\\n  --cluster-domain=cluster.local \\\\\n  --container-runtime=docker \\\\\n  --enable-custom-metrics \\\\\n  --kubeconfig=${KUBE_CERT_PATH}\/${NODE_NAME}.kubeconfig \\\\\n  --network-plugin=cni \\\\\n  --pod-cidr=${CLUSTER_SUBNET} \\\\\n  --register-node=true \\\\\n  --runtime-request-timeout=10m \\\\\n  --client-ca-file=${KUBE_CERT_PATH}\/ca.pem \\\\\n  --tls-cert-file=${KUBE_CERT_PATH}\/${NODE_NAME}.pem \\\\\n  --tls-private-key-file=${KUBE_CERT_PATH}\/${NODE_NAME}-key.pem \\\\\n  --pod-manifest-path=${KUBE_MANIFEST_PATH} \\\\\n  --read-only-port=0 \\\\\n  --protect-kernel-defaults=false \\\\\n  --make-iptables-util-chains=true \\\\\n  --keep-terminated-pod-volumes=false \\\\\n  --event-qps=0 \\\\\n  --cadvisor-port=0 \\\\\n  --runtime-cgroups=\/systemd\/system.slice \\\\\n  --kubelet-cgroups=\/systemd\/system.slice \\\\\n  --eviction-hard=memory.available<100Mi,nodefs.available<10%,nodefs.inodesFree<5% \\\\\n  --node-labels 'node-role.kubernetes.io\/worker=true' \\\\\n  --node-labels 'beta.kubernetes.io\/fluentd-ds-ready=true' \\\\\n  --v=2\nRestart=on-failure\nRestartSec=5\n\n[Install]\nWantedBy=multi-user.target\" | sudo -E tee \/etc\/systemd\/system\/kubelet.service\n\necho \"apiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-proxy\n  namespace: kube-system\n  labels:\n    k8s-app: kube-proxy\nspec:\n  hostNetwork: true\n  containers:\n  - name: kube-proxy\n    image: quay.io\/coreos\/hyperkube:${KUBELET_IMAGE_TAG}\n    command:\n    - \/hyperkube\n    - proxy\n    - --cluster-cidr=${CLUSTER_SUBNET}\n    - --masquerade-all=true\n    - --kubeconfig=${KUBE_CERT_PATH}\/kube-proxy.kubeconfig\n    - --proxy-mode=iptables\n    securityContext:\n      privileged: true\n    volumeMounts:\n    - mountPath: ${KUBE_CERT_PATH}\n      name: ssl-certs-kubernetes\n      readOnly: true\n    - mountPath: \/etc\/ssl\/certs\n      name: ssl-certs-host\n      readOnly: true\n  volumes:\n  - hostPath:\n      path: ${KUBE_CERT_PATH}\n    name: ssl-certs-kubernetes\n  - hostPath:\n      path: \/etc\/ssl\/certs\n    name: ssl-certs-host\" | sudo -E tee ${KUBE_MANIFEST_PATH}\/kube-proxy.yaml","script_type":"sh","type":""},"child_tasks_local_reference_list":[],"description":"","name":"Upgrade Minion","retries":"0","target_any_local_reference":{"kind":"app_service","name":"Kubernetes_Minion"},"timeout_secs":"0","type":"EXEC","variable_list":[]}],"variable_list":[{"attrs":{"type":""},"description":"","editables":{"value":true},"label":"","name":"KUBE_IMAGE_TAG_NEW","type":"LOCAL","val_type":"STRING","value":"v1.8.0_coreos.0"}]},"type":"user"}],"deployment_create_list":[{"action_list":[],"depends_on_list":[],"description":"","editables":{"max_replicas":true,"min_replicas":true},"max_replicas":"3","min_replicas":"3","name":"2cbb2f6a_deployment","package_local_reference_list":[{"kind":"app_package","name":"AHV_Centos_K8SC_Package"}],"published_service_local_reference_list":[],"substrate_local_reference":{"kind":"app_substrate","name":"AHV_Centos_K8SC"},"type":"GREENFIELD","variable_list":[]},{"action_list":[],"depends_on_list":[],"description":"","editables":{"max_replicas":true,"min_replicas":true},"max_replicas":"9","min_replicas":"3","name":"39cf7283_deployment","package_local_reference_list":[{"kind":"app_package","name":"AHV_Centos_K8SM_Package"}],"published_service_local_reference_list":[],"substrate_local_reference":{"kind":"app_substrate","name":"AHV_Centos_K8SM"},"type":"GREENFIELD","variable_list":[]}],"description":"","name":"Nutanix","variable_list":[{"attrs":{"type":""},"description":"","editables":{"value":true},"label":"","name":"DOCKER_VERSION","type":"LOCAL","val_type":"STRING","value":"17.03.2.ce"},{"attrs":{"type":""},"description":"","editables":{"value":true},"label":"","name":"INSTANCE_PUBLIC_KEY","type":"LOCAL","val_type":"STRING","value":"ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEApFiiPDXZ\/in4yknwl33rQkVi3Oo4X\/78ekW93+gGptbV1wCj\/PFay85\/aMA98ZYaAxXoC+MHe3QgUuXER45poH552IWdF0tU7X\/t0jH5KllVFx2yuFzCcjw1aKwdxKagDLX5jP4QvNsvB9pKx4dwECBp4qlKW4cpl\/2cWEpUqRi67PVDYr3TMt\/OCac6FaAMs5Du3N2HhGiWh2wezqUCRameLdni1\/G3ovaKpGL+c68n0peBsK63AJEa+AjVfpFcz6kzEUUMGSvAWD2d5nKCkJzqfBgh6kFJUTXPX0YnorbGsp1SuxmLy1pD7RXh2it5IPFUAFgEQfBwJJd7L4UkWQ== root@centos"},{"attrs":{"type":""},"description":"","editables":{"value":true},"label":"","name":"KUBE_CLUSTER_NAME","type":"LOCAL","val_type":"STRING","value":"kube-calm"},{"attrs":{"type":""},"description":"","editables":{"value":true},"label":"","name":"KUBE_CLUSTER_SUBNET","type":"LOCAL","val_type":"STRING","value":"10.200.0.0\/16"},{"attrs":{"type":""},"description":"","editables":{"value":true},"label":"","name":"KUBE_DNS_IP","type":"LOCAL","val_type":"STRING","value":"10.32.0.10"},{"attrs":{"type":""},"description":"","editables":{"value":true},"label":"","name":"KUBE_IMAGE_TAG","type":"LOCAL","val_type":"STRING","value":"v1.10.5_coreos.0"},{"attrs":{"type":""},"description":"","editables":{"value":true},"label":"","name":"KUBE_SERVICE_SUBNET","type":"LOCAL","val_type":"STRING","value":"10.32.0.0\/24"},{"attrs":{"type":""},"description":"","editables":{"value":true},"label":"","name":"PE_CLUSTER_IP","type":"LOCAL","val_type":"STRING","value":"10.132.129.37"},{"attrs":{"type":""},"description":"","editables":{"value":true},"label":"","name":"PE_CONTAINER_NAME","type":"LOCAL","val_type":"STRING","value":"SelfServiceContainer"},{"attrs":{"type":""},"description":"","editables":{"value":true},"label":"","name":"PE_DATA_SERVICE_IP","type":"LOCAL","val_type":"STRING","value":"10.132.129.38"},{"attrs":{"is_secret_modified":false,"secret_reference":{},"type":""},"description":"","editables":{"value":true},"label":"","name":"PE_PASSWORD","type":"SECRET","val_type":"STRING","value":""},{"attrs":{"type":""},"description":"","editables":{"value":true},"label":"","name":"PE_USERNAME","type":"LOCAL","val_type":"STRING","value":"admin"}]}],"client_attrs":{"2cbb2f6a_deployment":{"x":-166,"y":-68},"39cf7283_deployment":{"x":118,"y":104},"None":{"x":-166,"y":-68}},"credential_definition_list":[{"description":"","name":"CENTOS","secret":{"attrs":{"is_secret_modified":false,"secret_reference":{}}},"type":"KEY","username":"centos"}],"default_credential_local_reference":{"kind":"app_credential","name":"CENTOS"},"package_definition_list":[{"action_list":[],"description":"","name":"AHV_Centos_K8SC_Package","options":{"install_runbook":{"description":"","main_task_local_reference":{"kind":"app_task","name":"dac441af_dag"},"message_list":[],"name":"8e760b73_runbook","state":"ACTIVE","task_definition_list":[{"attrs":{"command_line_args":"","exit_status":[],"script":"#!\/bin\/bash\nset -ex\n\nMASTER_API_HTTPS=6443\nINTERNAL_IP=\"@@{address}@@\"\nKUBE_CLUSTER_NAME=\"@@{KUBE_CLUSTER_NAME}@@\"\n\nsudo -E systemctl start etcd docker kubelet iscsid\nsudo -E systemctl enable etcd docker kubelet iscsid\n\nexport PATH=$PATH:\/opt\/bin\n\nmkdir CA\nmv admin*.pem ca*.pem etcd-*.pem kubernetes*.pem controller* kube-*.kubeconfig encryption-config.yaml CA\/\nif [ @@{calm_array_index}@@ -ne 0 ];then\n  exit\nfi\ncp \/opt\/kube-ssl\/admin*.pem CA\/\nCOUNT=0\nwhile [[ $(curl --key CA\/admin-key.pem --cert CA\/admin.pem --cacert CA\/ca.pem https:\/\/${INTERNAL_IP}:${MASTER_API_HTTPS}\/healthz) != \"ok\" ]] ; do\n    echo \"sleep for 5 secs\"\n  sleep 60\n  COUNT=$(($COUNT+1))\n  if [[ $COUNT -eq 60 ]]; then\n  \techo \"Error: creating cluster\"\n    exit 1\n  fi\ndone\n\nkubectl config set-cluster ${KUBE_CLUSTER_NAME}  --certificate-authority=$HOME\/CA\/ca.pem  --embed-certs=true --server=https:\/\/${INTERNAL_IP}:${MASTER_API_HTTPS}\nkubectl config set-credentials admin  --client-certificate=$HOME\/CA\/admin.pem  --client-key=$HOME\/CA\/admin-key.pem\nkubectl config set-context ${KUBE_CLUSTER_NAME}  --cluster=${KUBE_CLUSTER_NAME}  --user=admin\nkubectl config use-context ${KUBE_CLUSTER_NAME}\n\ncat <<EOF | kubectl apply -f -\napiVersion: rbac.authorization.k8s.io\/v1beta1\nkind: ClusterRole\nmetadata:\n  annotations:\n    rbac.authorization.kubernetes.io\/autoupdate: \"true\"\n  labels:\n    kubernetes.io\/bootstrapping: rbac-defaults\n  name: system:kube-apiserver-to-kubelet\nrules:\n  - apiGroups:\n      - \"\"\n    resources:\n      - nodes\/proxy\n      - nodes\/stats\n      - nodes\/log\n      - nodes\/spec\n      - nodes\/metrics\n    verbs:\n      - \"*\"\nEOF\n\ncat <<EOF | kubectl apply -f -\napiVersion: rbac.authorization.k8s.io\/v1beta1\nkind: ClusterRoleBinding\nmetadata:\n  name: system:kube-apiserver\n  namespace: \"\"\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:kube-apiserver-to-kubelet\nsubjects:\n  - apiGroup: rbac.authorization.k8s.io\n    kind: User\n    name: kubernetes\nEOF","script_type":"sh","type":""},"child_tasks_local_reference_list":[],"description":"","message_list":[],"name":"Add User Roles","retries":"0","state":"ACTIVE","target_any_local_reference":{"kind":"app_service","name":"Kubernetes_Master"},"timeout_secs":"0","type":"EXEC","variable_list":[]},{"attrs":{"command_line_args":"","exit_status":[],"script":"#!\/bin\/bash\nset -ex\n\nKUBELET_IMAGE_TAG=\"@@{KUBE_IMAGE_TAG}@@\"\nif [[ \"@@{KUBE_IMAGE_TAG_NEW}@@x\" != \"x\" ]]; then\n\tKUBELET_IMAGE_TAG=\"@@{KUBE_IMAGE_TAG_NEW}@@\"\nfi\nINTERNAL_IP=\"@@{address}@@\"\nCONTROLLER_IPS=\"@@{calm_array_address}@@\"\nCLUSTER_SUBNET=\"@@{KUBE_CLUSTER_SUBNET}@@\"\nSERVICE_SUBNET=\"@@{KUBE_SERVICE_SUBNET}@@\"\nETCD_CERT_PATH=\"\/etc\/ssl\/certs\/etcd\"\nKUBE_CERT_PATH=\"\/etc\/kubernetes\/ssl\"\nKUBE_MANIFEST_PATH=\"\/etc\/kubernetes\/manifests\"\nMASTER_API_HTTPS=6443\nETCD_SERVER_PORT=2379\nVERSION=$(echo \"${KUBELET_IMAGE_TAG}\" | tr \"_\" \" \" | awk '{print $1}')\nCONTROLLER_COUNT=$(echo \"@@{calm_array_address}@@\" | tr ',' '\\n' | wc -l)\n\nsudo -E cp ca*.pem etcd-*.pem kubernetes*.pem ${HOSTNAME}* kube-*.kubeconfig encryption-config.yaml ${KUBE_CERT_PATH}\/\nsudo -E chmod +r ${KUBE_CERT_PATH}\/*\n\nsudo -E cp etcd-*.pem ${ETCD_CERT_PATH}\/\nsudo -E chmod +r ${ETCD_CERT_PATH}\/*\n\ncount=0\nfor ip in $(echo \"${CONTROLLER_IPS}\" | tr \",\" \"\\n\"); do\n  ETCD+=\"https:\/\/${ip}:${ETCD_SERVER_PORT}\",\n  count=$((count+1))\ndone\nETCD_SERVERS=$(echo $ETCD | sed  's\/,$\/\/')\n\necho \"apiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-apiserver\n  namespace: kube-system\n  labels:\n    k8s-app: kube-apiserver\nspec:\n  hostNetwork: true\n  containers:\n  - name: kube-apiserver\n    image: quay.io\/coreos\/hyperkube:${KUBELET_IMAGE_TAG}\n    imagePullPolicy: IfNotPresent\n    command:\n    - \/hyperkube\n    - apiserver\n    - --admission-control=Initializers,NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota\n    - --advertise-address=${INTERNAL_IP}\n    - --allow-privileged=true\n    - --anonymous-auth=false\n    - --insecure-port=0\n    - --secure-port=${MASTER_API_HTTPS}\n    - --profiling=false\n    - --repair-malformed-updates=false\n    - --apiserver-count=${CONTROLLER_COUNT}\n    - --audit-log-maxage=30\n    - --audit-log-maxbackup=10\n    - --audit-log-maxsize=100\n    - --audit-log-path=\/var\/lib\/audit.log\n    - --authorization-mode=Node,RBAC\n    - --bind-address=0.0.0.0\n    - --kubelet-preferred-address-types=InternalIP,Hostname,ExternalIP\n    - --event-ttl=1h\n    - --service-account-lookup=true\n    - --enable-swagger-ui=true\n    - --storage-backend=etcd3\n    - --etcd-cafile=${ETCD_CERT_PATH}\/etcd-ca.pem\n    - --etcd-certfile=${ETCD_CERT_PATH}\/etcd-client.pem\n    - --etcd-keyfile=${ETCD_CERT_PATH}\/etcd-client-key.pem\n    - --etcd-servers=${ETCD_SERVERS}\n    - --experimental-encryption-provider-config=${KUBE_CERT_PATH}\/encryption-config.yaml\n    - --tls-ca-file=${KUBE_CERT_PATH}\/ca.pem\n    - --tls-cert-file=${KUBE_CERT_PATH}\/kubernetes.pem\n    - --tls-private-key-file=${KUBE_CERT_PATH}\/kubernetes-key.pem\n    - --kubelet-client-certificate=${KUBE_CERT_PATH}\/kubernetes.pem\n    - --kubelet-client-key=${KUBE_CERT_PATH}\/kubernetes-key.pem\n    - --kubelet-https=true\n    - --runtime-config=api\/all\n    - --service-account-key-file=${KUBE_CERT_PATH}\/kubernetes.pem\n    - --service-cluster-ip-range=${SERVICE_SUBNET}\n    - --service-node-port-range=30000-32767\n    - --client-ca-file=${KUBE_CERT_PATH}\/ca.pem\n    - --v=2\n    ports:\n    - containerPort: ${MASTER_API_HTTPS}\n      hostPort: ${MASTER_API_HTTPS}\n      name: https\n    - containerPort: 8080\n      hostPort: 8080\n      name: local\n    volumeMounts:\n    - mountPath: ${KUBE_CERT_PATH}\n      name: ssl-certs-kubernetes\n      readOnly: true\n    - mountPath: \/etc\/ssl\/certs\n      name: ssl-certs-host\n      readOnly: true\n    - mountPath: \/etc\/pki\n      name: ca-certs-etc-pki\n      readOnly: true\n  volumes:\n  - hostPath:\n      path: ${KUBE_CERT_PATH}\n    name: ssl-certs-kubernetes\n  - hostPath:\n      path: \/etc\/ssl\/certs\n    name: ssl-certs-host\n  - hostPath:\n      path: \/etc\/pki\n    name: ca-certs-etc-pki\" | sudo -E tee ${KUBE_MANIFEST_PATH}\/kube-apiserver.yaml\n\necho \"apiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-proxy\n  namespace: kube-system\n  labels:\n    k8s-app: kube-proxy\nspec:\n  hostNetwork: true\n  containers:\n  - name: kube-proxy\n    image: quay.io\/coreos\/hyperkube:${KUBELET_IMAGE_TAG}\n    imagePullPolicy: IfNotPresent\n    command:\n    - \/hyperkube\n    - proxy\n    - --cluster-cidr=${CLUSTER_SUBNET}\n    - --masquerade-all=true\n    - --kubeconfig=${KUBE_CERT_PATH}\/kube-proxy.kubeconfig\n    - --proxy-mode=iptables\n    securityContext:\n      privileged: true\n    volumeMounts:\n    - mountPath: ${KUBE_CERT_PATH}\n      name: ssl-certs-kubernetes\n      readOnly: true\n    - mountPath: \/etc\/ssl\/certs\n      name: ssl-certs-host\n      readOnly: true\n  volumes:\n  - hostPath:\n      path: ${KUBE_CERT_PATH}\n    name: ssl-certs-kubernetes\n  - hostPath:\n      path: \/etc\/ssl\/certs\n    name: ssl-certs-host\" | sudo -E tee ${KUBE_MANIFEST_PATH}\/kube-proxy.yaml\n    \necho \"apiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-controller-manager\n  namespace: kube-system\n  labels:\n    k8s-app: kube-controller-manager\nspec:\n  hostNetwork: true\n  containers:\n  - name: kube-controller-manager\n    image: quay.io\/coreos\/hyperkube:${KUBELET_IMAGE_TAG}\n    imagePullPolicy: IfNotPresent\n    command:\n    - \/hyperkube\n    - controller-manager\n    - --address=0.0.0.0  \n    - --allocate-node-cidrs=true  \n    - --cluster-cidr=${CLUSTER_SUBNET}\n    - --cluster-name=kubernetes-prod-cluster  \n    - --leader-elect=true  \n    - --kubeconfig=${KUBE_CERT_PATH}\/kube-controller-manager.kubeconfig  \n    - --service-account-private-key-file=${KUBE_CERT_PATH}\/kubernetes-key.pem  \n    - --service-cluster-ip-range=${SERVICE_SUBNET}\n    - --terminated-pod-gc-threshold=100  \n    - --profiling=false  \n    - --use-service-account-credentials=true  \n    - --v=2\n    livenessProbe:\n      httpGet:\n        host: 127.0.0.1\n        path: \/healthz\n        port: 10252\n      initialDelaySeconds: 15\n      timeoutSeconds: 1\n    volumeMounts:\n    - mountPath: ${KUBE_CERT_PATH}\n      name: ssl-certs-kubernetes\n      readOnly: true\n    - mountPath: \/etc\/ssl\/certs\n      name: ssl-certs-host\n      readOnly: true\n    - mountPath: \/etc\/pki\n      name: ca-certs-etc-pki\n      readOnly: true\n  volumes:\n  - hostPath:\n      path: ${KUBE_CERT_PATH}\n    name: ssl-certs-kubernetes\n  - hostPath:\n      path: \/etc\/ssl\/certs\n    name: ssl-certs-host\n  - hostPath:\n      path: \/etc\/pki\n    name: ca-certs-etc-pki\" | sudo -E tee ${KUBE_MANIFEST_PATH}\/kube-controller-manager.yaml\n    \necho \"apiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-scheduler\n  namespace: kube-system\n  labels:\n    k8s-app: kube-scheduler\nspec:\n  hostNetwork: true\n  containers:\n  - name: kube-scheduler\n    image: quay.io\/coreos\/hyperkube:${KUBELET_IMAGE_TAG}\n    imagePullPolicy: IfNotPresent\n    command:\n    - \/hyperkube\n    - scheduler\n    - --kubeconfig=${KUBE_CERT_PATH}\/kube-scheduler.kubeconfig\n    - --leader-elect=true\n    - --profiling=false\n    - --v=2\n    livenessProbe:\n      httpGet:\n        host: 127.0.0.1\n        path: \/healthz\n        port: 10251\n      initialDelaySeconds: 15\n      timeoutSeconds: 1\n    volumeMounts:\n    - mountPath: ${KUBE_CERT_PATH}\n      name: ssl-certs-kubernetes\n      readOnly: true\n  volumes:\n  - hostPath:\n      path: ${KUBE_CERT_PATH}\n    name: ssl-certs-kubernetes\" | sudo -E tee ${KUBE_MANIFEST_PATH}\/kube-scheduler.yaml","script_type":"sh","type":""},"child_tasks_local_reference_list":[],"description":"","message_list":[],"name":"Configure Services","retries":"0","state":"ACTIVE","target_any_local_reference":{"kind":"app_service","name":"Kubernetes_Master"},"timeout_secs":"0","type":"EXEC","variable_list":[]},{"attrs":{"command_line_args":"","exit_status":[],"script":"#!\/bin\/bash\nset -ex\n\nif [ @@{calm_array_index}@@ -ne 0 ];then\n\texit\nfi\nexport PATH=$PATH:\/opt\/bin\n\nsudo -E mkdir \/etc\/kubernetes\/addons\/kubedns\necho 'apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: kube-dns\n  namespace: kube-system\n  labels:\n    kubernetes.io\/cluster-service: \"true\"\n    addonmanager.kubernetes.io\/mode: Reconcile\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: kube-dns\n  namespace: kube-system\n  labels:\n    addonmanager.kubernetes.io\/mode: EnsureExists\ndata:\n  upstreamNameservers: |\n    [\"8.8.8.8\", \"4.2.2.2\"]\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: kube-dns\n  namespace: kube-system\n  labels:\n    k8s-app: kube-dns\n    kubernetes.io\/cluster-service: \"true\"\n    kubernetes.io\/name: \"KubeDNS\"\nspec:\n  selector:\n    k8s-app: kube-dns\n  clusterIP: @@{KUBE_DNS_IP}@@\n  ports:\n    - name: dns\n      port: 53\n      protocol: UDP\n    - name: dns-tcp\n      port: 53\n      protocol: TCP\n---\napiVersion: apps\/v1beta2 #extensions\/v1beta1\nkind: Deployment\nmetadata:\n  name: kube-dns\n  namespace: kube-system\n  labels:\n    k8s-app: kube-dns\n    kubernetes.io\/cluster-service: \"true\"\n    addonmanager.kubernetes.io\/mode: Reconcile\nspec:\n  strategy:\n    rollingUpdate:\n      maxSurge: 10%\n      maxUnavailable: 0\n  selector:\n    matchLabels:\n      k8s-app: kube-dns\n  template:\n    metadata:\n      labels:\n        k8s-app: kube-dns\n      annotations:\n        scheduler.alpha.kubernetes.io\/critical-pod: \"\"\n    spec:\n      tolerations:\n      - key: \"CriticalAddonsOnly\"\n        operator: \"Exists\"\n      volumes:\n      - name: kube-dns-config\n        configMap:\n          name: kube-dns\n          optional: true\n      containers:\n      - name: kubedns\n        image: gcr.io\/google_containers\/k8s-dns-kube-dns-amd64:1.14.8\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            memory: 170Mi\n          requests:\n            cpu: 100m\n            memory: 70Mi\n        livenessProbe:\n          httpGet:\n            path: \/healthcheck\/kubedns\n            port: 10054\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: \/readiness\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 3\n          timeoutSeconds: 5\n        args:\n        - --domain=cluster.local.\n        - --dns-port=10053\n        - --config-dir=\/kube-dns-config\n        - --v=2\n        env:\n        - name: PROMETHEUS_PORT\n          value: \"10055\"\n        ports:\n        - containerPort: 10053\n          name: dns-local\n          protocol: UDP\n        - containerPort: 10053\n          name: dns-tcp-local\n          protocol: TCP\n        - containerPort: 10055\n          name: metrics\n          protocol: TCP\n        volumeMounts:\n        - name: kube-dns-config\n          mountPath: \/kube-dns-config\n      - name: dnsmasq\n        image: gcr.io\/google_containers\/k8s-dns-dnsmasq-nanny-amd64:1.14.8\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: \/healthcheck\/dnsmasq\n            port: 10054\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        args:\n        - -v=2\n        - -logtostderr\n        - -configDir=\/etc\/k8s\/dns\/dnsmasq-nanny\n        - -restartDnsmasq=true\n        - --\n        - -k\n        - --cache-size=1000\n        - --log-facility=-\n        - --server=\/cluster.local.\/127.0.0.1#10053\n        - --server=\/in-addr.arpa\/127.0.0.1#10053\n        - --server=\/ip6.arpa\/127.0.0.1#10053\n        ports:\n        - containerPort: 53\n          name: dns\n          protocol: UDP\n        - containerPort: 53\n          name: dns-tcp\n          protocol: TCP\n        resources:\n          requests:\n            cpu: 150m\n            memory: 20Mi\n        volumeMounts:\n        - name: kube-dns-config\n          mountPath: \/etc\/k8s\/dns\/dnsmasq-nanny\n      - name: sidecar\n        image: gcr.io\/google_containers\/k8s-dns-sidecar-amd64:1.14.8\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: \/metrics\n            port: 10054\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        args:\n        - --v=2\n        - --logtostderr\n        - --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local.,5,A\n        - --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local.,5,A\n        ports:\n        - containerPort: 10054\n          name: metrics\n          protocol: TCP\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n      dnsPolicy:\n      serviceAccountName: kube-dns' | sudo -E tee \/etc\/kubernetes\/addons\/kubedns\/kube-dns.yaml\n      \necho 'kind: ServiceAccount\napiVersion: v1\nmetadata:\n  name: kube-dns-autoscaler\n  namespace: kube-system\n  labels:\n    addonmanager.kubernetes.io\/mode: Reconcile\n---\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io\/v1\nmetadata:\n  name: system:kube-dns-autoscaler\n  labels:\n    addonmanager.kubernetes.io\/mode: Reconcile\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"nodes\"]\n    verbs: [\"list\"]\n  - apiGroups: [\"\"]\n    resources: [\"replicationcontrollers\/scale\"]\n    verbs: [\"get\", \"update\"]\n  - apiGroups: [\"extensions\"]\n    resources: [\"deployments\/scale\", \"replicasets\/scale\"]\n    verbs: [\"get\", \"update\"]\n# Remove the configmaps rule once below issue is fixed:\n# kubernetes-incubator\/cluster-proportional-autoscaler#16\n  - apiGroups: [\"\"]\n    resources: [\"configmaps\"]\n    verbs: [\"get\", \"create\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io\/v1\nmetadata:\n  name: system:kube-dns-autoscaler\n  labels:\n    addonmanager.kubernetes.io\/mode: Reconcile\nsubjects:\n  - kind: ServiceAccount\n    name: kube-dns-autoscaler\n    namespace: kube-system\nroleRef:\n  kind: ClusterRole\n  name: system:kube-dns-autoscaler\n  apiGroup: rbac.authorization.k8s.io\n\n---\napiVersion: apps\/v1beta2 #extensions\/v1beta1\nkind: Deployment\nmetadata:\n  name: kube-dns-autoscaler\n  namespace: kube-system\n  labels:\n    k8s-app: kube-dns-autoscaler\n    kubernetes.io\/cluster-service: \"true\"\n    addonmanager.kubernetes.io\/mode: Reconcile\nspec:\n  selector:\n    matchLabels:\n      k8s-app: kube-dns-autoscaler\n  template:\n    metadata:\n      labels:\n        k8s-app: kube-dns-autoscaler\n      annotations:\n        scheduler.alpha.kubernetes.io\/critical-pod: \"\"\n    spec:\n      priorityClassName: system-cluster-critical\n      containers:\n      - name: autoscaler\n        image: k8s.gcr.io\/cluster-proportional-autoscaler-amd64:1.1.2-r2\n        imagePullPolicy: IfNotPresent\n        resources:\n            requests:\n                cpu: \"20m\"\n                memory: \"10Mi\"\n        command:\n          - \/cluster-proportional-autoscaler\n          - --namespace=kube-system\n          - --configmap=kube-dns-autoscaler\n          # Should keep target in sync with cluster\/addons\/dns\/kube-dns.yaml.base\n          - --target=Deployment\/kube-dns\n          # When cluster is using large nodes(with more cores), \"coresPerReplica\" should dominate.\n          # If using small nodes, \"nodesPerReplica\" should dominate.\n          - --default-params={\"linear\":{\"coresPerReplica\":256,\"nodesPerReplica\":16,\"preventSinglePointFailure\":true}}\n          - --logtostderr=true\n          - --v=2\n      tolerations:\n      - key: \"CriticalAddonsOnly\"\n        operator: \"Exists\"\n      serviceAccountName: kube-dns-autoscaler' | sudo -E tee \/etc\/kubernetes\/addons\/kubedns\/kube-dns-autoscaler.yaml\n\nkubectl create -f \/etc\/kubernetes\/addons\/kubedns\/kube-dns.yaml\nkubectl create -f \/etc\/kubernetes\/addons\/kubedns\/kube-dns-autoscaler.yaml\n","script_type":"sh","type":""},"child_tasks_local_reference_list":[],"description":"","message_list":[],"name":"DNS Configuration","retries":"0","state":"ACTIVE","target_any_local_reference":{"kind":"app_service","name":"Kubernetes_Master"},"timeout_secs":"0","type":"EXEC","variable_list":[]},{"attrs":{"command_line_args":"","exit_status":[],"script":"#!\/bin\/bash\nset -ex\n\n#sudo -E easy_install netaddr\n\nETCD_VERSION=\"v3.2.24\" #defualt v3.2.11\nKUBELET_IMAGE_TAG=\"@@{KUBE_IMAGE_TAG}@@\"\nif [[ \"@@{KUBE_IMAGE_TAG_NEW}@@x\" != \"x\" ]]; then\n\tKUBELET_IMAGE_TAG=\"@@{KUBE_IMAGE_TAG_NEW}@@\"\nfi\nINTERNAL_IP=\"@@{address}@@\"\nCONTROLLER_IPS=\"@@{calm_array_address}@@\"\nMINION_IPS=\"@@{AHV_Centos_K8SM.address}@@\"\nNODE_NAME=\"controller@@{calm_array_index}@@\"\nCLUSTER_SUBNET=\"@@{KUBE_CLUSTER_SUBNET}@@\"\nSERVICE_SUBNET=\"@@{KUBE_SERVICE_SUBNET}@@\"\nKUBE_CLUSTER_DNS=\"@@{KUBE_DNS_IP}@@\"\nDOCKER_VERSION=\"@@{DOCKER_VERSION}@@\"\nETCD_CERT_PATH=\"\/etc\/ssl\/certs\/etcd\"\nKUBE_CERT_PATH=\"\/etc\/kubernetes\/ssl\"\nKUBE_MANIFEST_PATH=\"\/etc\/kubernetes\/manifests\"\nKUBE_CNI_BIN_PATH=\"\/opt\/cni\/bin\"\nKUBE_CNI_CONF_PATH=\"\/etc\/cni\/net.d\"\nVERSION=$(echo \"${KUBELET_IMAGE_TAG}\" | tr \"_\" \" \" | awk '{print $1}')\nMASTER_API_HTTPS=6443\nETCD_SERVER_PORT=2379\nETCD_CLIENT_PORT=2380\nMASTER_API_PORT=8080\nFIRST_IP_SERVICE_SUBNET=$(python -c \"from netaddr import * ; print IPNetwork('${SERVICE_SUBNET}')[1]\")\n\nsudo -E mkdir -p \/opt\/kube-ssl ${KUBE_CERT_PATH} ${KUBE_CNI_BIN_PATH} ${ETCD_CERT_PATH} ${KUBE_MANIFEST_PATH} ${KUBE_CNI_CONF_PATH}\n\nsudo -E hostnamectl set-hostname --static ${NODE_NAME}\n#sudo -E yum update -y --quiet\nsudo -E yum install -y wget iscsi-initiator-utils socat --quiet\n\ncount=0\nfor ip in $(echo \"${CONTROLLER_IPS}\" | tr \",\" \"\\n\"); do\n  echo \"${ip} controller${count}\" | sudo -E tee -a \/etc\/hosts\n  CON+=\"controller${count}=https:\/\/${ip}:${ETCD_CLIENT_PORT}\",\n  ETCD+=\"https:\/\/${ip}:${ETCD_SERVER_PORT}\",\n  CONS_NAMES+=\"controller${count}\",\n  count=$((count+1))\ndone\nETCD_ALL_CONTROLLERS=$(echo $CON | sed  's\/,$\/\/')\nETCD_SERVERS=$(echo $ETCD | sed  's\/,$\/\/')\nCONTROLLER_NAMES=$(echo $CONS_NAMES | sed  's\/,$\/\/')\n  \ncount=0\nfor ip in $(echo ${MINION_IPS} | tr \",\" \"\\n\"); do\n  echo \"${ip} minion${count}\" | sudo -E tee -a \/etc\/hosts\n  MIN_NAMES+=\"minion${count}\",\n  count=$((count+1))\ndone\nMINION_NAMES=$(echo $MIN_NAMES | sed  's\/,$\/\/')    \n    \n#wget -q \"https:\/\/github.com\/coreos\/etcd\/releases\/download\/${ETCD_VERSION}\/etcd-${ETCD_VERSION}-linux-amd64.tar.gz\"\n#wget -q https:\/\/github.com\/containernetworking\/plugins\/releases\/download\/v0.6.0\/cni-plugins-amd64-v0.6.0.tgz\n#wget -q https:\/\/storage.googleapis.com\/kubernetes-release\/release\/${VERSION}\/bin\/linux\/amd64\/kubelet\nchmod +x kubelet\nsudo -E mv kubelet \/usr\/bin\/kubelet\n\n# -*- Bootstrapping a H\/A etcd cluster.\ntar -xvf etcd-${ETCD_VERSION}-linux-amd64.tar.gz\nsudo -E mv etcd-${ETCD_VERSION}-linux-amd64\/etcd* \/usr\/bin\/\nrm -rf etcd-${ETCD_VERSION}-linux-amd64*\n\necho \"[Unit]\nDescription=etcd\nDocumentation=https:\/\/github.com\/coreos\n\n[Service]\nExecStart=\/usr\/bin\/etcd \\\\\n  --name ${NODE_NAME} \\\\\n  --cert-file=${ETCD_CERT_PATH}\/etcd-server.pem \\\\\n  --key-file=${ETCD_CERT_PATH}\/etcd-server-key.pem \\\\\n  --peer-cert-file=${ETCD_CERT_PATH}\/etcd-peer.pem \\\\\n  --peer-key-file=${ETCD_CERT_PATH}\/etcd-peer-key.pem \\\\\n  --trusted-ca-file=${ETCD_CERT_PATH}\/etcd-ca.pem \\\\\n  --peer-trusted-ca-file=${ETCD_CERT_PATH}\/etcd-ca.pem \\\\\n  --peer-client-cert-auth \\\\\n  --client-cert-auth \\\\\n  --initial-advertise-peer-urls https:\/\/${INTERNAL_IP}:${ETCD_CLIENT_PORT} \\\\\n  --listen-peer-urls https:\/\/${INTERNAL_IP}:${ETCD_CLIENT_PORT} \\\\\n  --listen-client-urls https:\/\/${INTERNAL_IP}:${ETCD_SERVER_PORT},http:\/\/127.0.0.1:${ETCD_SERVER_PORT} \\\\\n  --advertise-client-urls https:\/\/${INTERNAL_IP}:${ETCD_SERVER_PORT} \\\\\n  --initial-cluster-token etcd-cluster-0 \\\\\n  --initial-cluster ${ETCD_ALL_CONTROLLERS} \\\\\n  --initial-cluster-state new \\\\\n  --data-dir=\/var\/lib\/etcd \\\\\n  --wal-dir=\/var\/lib\/etcd\/wal \\\\\n  --max-wals=0\nRestart=on-failure\nRestartSec=5\n\n[Install]\nWantedBy=multi-user.target\" | sudo -E tee \/etc\/systemd\/system\/etcd.service\n\nsudo -E yum install -y --quiet yum-utils\n\n#sudo -E yum-config-manager --add-repo https:\/\/download.docker.com\/linux\/centos\/docker-ce.repo\n#sudo -E yum install -y --quiet --setopt=obsoletes=0 docker-ce-${DOCKER_VERSION} docker-ce-selinux-${DOCKER_VERSION}\n\n#sudo -E sed -i '\/ExecStart=\/c\\\\ExecStart=\/usr\/bin\/dockerd -H tcp:\/\/0.0.0.0:2375 -H unix:\/\/\/var\/run\/docker.sock' \/usr\/lib\/systemd\/system\/docker.service\n\n#cp \/usr\/lib\/systemd\/system\/docker.service \/tmp\n#sudo -E sed -i '\/\\[Service\\]\/c\\\\[Service]\\nEnvironment=\\\"HTTP_PROXY=http:\/\/192.168.0.209:1080\/\\\"' \/usr\/lib\/systemd\/system\/docker.service\n\nsudo -E systemctl enable docker\nsudo -E usermod -a -G docker $USER\n\n#sudo -E mkdir -p \/etc\/docker\n#echo '{\n#  \"storage-driver\": \"overlay\"\n#}' | sudo -E tee \/etc\/docker\/daemon.json\n\necho '{\n  \"name\": \"cbr0\",\n  \"type\": \"flannel\",\n  \"delegate\": {\n    \"isDefaultGateway\": true\n  }\n}' | sudo -E tee ${KUBE_CNI_CONF_PATH}\/10-flannel.conf\n\nsudo -E tar -zxvf cni-plugins-amd64-v0.6.0.tgz -C ${KUBE_CNI_BIN_PATH}\nrm -rf cni-plugins-amd64-v0.6.0.tgz\n\necho \"[Unit]\nDescription=Kubernetes Kubelet\nDocumentation=https:\/\/github.com\/GoogleCloudPlatform\/kubernetes\nAfter=docker.service\nRequires=docker.service\n\n[Service]\nExecStart=\/usr\/bin\/kubelet \\\\\n  --allow-privileged=true \\\\\n  --anonymous-auth=false \\\\\n  --authorization-mode=Webhook \\\\\n  --cluster-dns=${KUBE_CLUSTER_DNS} \\\\\n  --cluster-domain=cluster.local \\\\\n  --container-runtime=docker \\\\\n  --enable-custom-metrics \\\\\n  --kubeconfig=${KUBE_CERT_PATH}\/${NODE_NAME}.kubeconfig \\\\\n  --network-plugin=cni \\\\\n  --pod-cidr=${CLUSTER_SUBNET} \\\\\n  --register-node=true \\\\\n  --runtime-request-timeout=10m \\\\\n  --client-ca-file=${KUBE_CERT_PATH}\/ca.pem \\\\\n  --tls-cert-file=${KUBE_CERT_PATH}\/${NODE_NAME}.pem \\\\\n  --tls-private-key-file=${KUBE_CERT_PATH}\/${NODE_NAME}-key.pem \\\\\n  --pod-manifest-path=${KUBE_MANIFEST_PATH} \\\\\n  --read-only-port=0 \\\\\n  --protect-kernel-defaults=false \\\\\n  --make-iptables-util-chains=true \\\\\n  --keep-terminated-pod-volumes=false \\\\\n  --event-qps=0 \\\\\n  --cadvisor-port=0 \\\\\n  --runtime-cgroups=\/systemd\/system.slice \\\\\n  --kubelet-cgroups=\/systemd\/system.slice \\\\\n  --node-labels 'node-role.kubernetes.io\/master=true' \\\\\n  --node-labels 'node-role.kubernetes.io\/etcd=true' \\\\\n  --register-with-taints=node-role.kubernetes.io\/master=true:NoSchedule \\\\\n  --v=2\nRestart=on-failure\nRestartSec=5\n\n[Install]\nWantedBy=multi-user.target\" | sudo -E tee \/etc\/systemd\/system\/kubelet.service\n\n#sudo -E mkdir -p \/var\/lib\/docker\n#sudo -E yum install -y lvm2 --quiet\n#sudo -E pvcreate \/dev\/sd{b,c,d}\n#sudo -E vgcreate docker \/dev\/sd{b,c,d}\n\n#sleep 10\n#sudo -E udevadm control --reload-rules\n\n#sudo -E lvcreate -l 100%VG -n docker_lvm docker\n#sudo -E mkfs.xfs \/dev\/docker\/docker_lvm\n\n#echo -e \"\/dev\/docker\/docker_lvm \\t \/var\/lib\/docker \\t xfs \\t defaults \\t 0 0\" | sudo -E tee -a \/etc\/fstab\n#sudo -E mount -a\necho \"InitiatorName=iqn.1994-05.com.nutanix:k8s-worker\" | sudo -E tee \/etc\/iscsi\/initiatorname.iscsi\necho 'exclude=docker*' | sudo -E tee -a \/etc\/yum.conf\n\n#wget -q https:\/\/pkg.cfssl.org\/R1.2\/cfssl_linux-amd64 https:\/\/pkg.cfssl.org\/R1.2\/cfssljson_linux-amd64\n#wget -q https:\/\/storage.googleapis.com\/kubernetes-release\/release\/${VERSION}\/bin\/linux\/amd64\/kubectl\n#wget -q \"https:\/\/storage.googleapis.com\/kubernetes-helm\/helm-v2.8.1-linux-amd64.tar.gz\"\n\ntar -zxvf helm-v2.8.2-linux-amd64.tar.gz\nchmod +x cfssl_linux-amd64 cfssljson_linux-amd64 kubectl linux-amd64\/helm\nsudo -E mv cfssl_linux-amd64 \/usr\/local\/bin\/cfssl\nsudo -E mv cfssljson_linux-amd64 \/usr\/local\/bin\/cfssljson\nsudo -E mv kubectl linux-amd64\/helm \/usr\/local\/bin\/\nrm -rf helm-v2.8.2-linux-amd64.tar.gz linux-amd64","script_type":"sh","type":""},"child_tasks_local_reference_list":[],"description":"","message_list":[],"name":"ETCD Docker Kubelet Install","retries":"0","state":"ACTIVE","target_any_local_reference":{"kind":"app_service","name":"Kubernetes_Master"},"timeout_secs":"0","type":"EXEC","variable_list":[]},{"attrs":{"command_line_args":"","exit_status":[],"script":"#!\/bin\/bash\nset -ex\n\nINTERNAL_IP=\"@@{address}@@\"\nCONTROLLER_IPS=\"@@{calm_array_address}@@\"\nMINION_IPS=\"@@{AHV_Centos_K8SM.address}@@\"\nMASTER_API_HTTPS=6443\nSERVICE_SUBNET=\"@@{KUBE_SERVICE_SUBNET}@@\"\nKUBE_CLUSTER_NAME=\"@@{KUBE_CLUSTER_NAME}@@\"\nFIRST_IP_SERVICE_SUBNET=$(python -c \"from netaddr import * ; print IPNetwork('${SERVICE_SUBNET}')[1]\")\n\ncount=0\nfor ip in $(echo \"${CONTROLLER_IPS}\" | tr \",\" \"\\n\"); do\n  CONS_NAMES+=\"controller${count}\",\n  count=$((count+1))\ndone\n\nCONTROLLER_NAMES=$(echo $CONS_NAMES | sed  's\/,$\/\/')\n  \ncount=0\nfor ip in $(echo ${MINION_IPS} | tr \",\" \"\\n\"); do\n  MIN_NAMES+=\"minion${count}\",\n  count=$((count+1))\ndone\nMINION_NAMES=$(echo $MIN_NAMES | sed  's\/,$\/\/')  \n\nif [ @@{calm_array_index}@@ -ne 0 ];then\n  exit\nfi\nsudo -E chown -R $USER:$USER \/opt\/kube-ssl && cd \/opt\/kube-ssl\necho '{\n  \"signing\": {\n    \"default\": {\n      \"expiry\": \"8760h\"\n    },\n    \"profiles\": {\n      \"server\": {\n        \"expiry\": \"8760h\",\n        \"usages\": [ \"signing\", \"key encipherment\", \"server auth\", \"client auth\" ]\n      },\n      \"client\": {\n        \"expiry\": \"8760h\",\n        \"usages\": [ \"key encipherment\", \"client auth\" ]\n      },\n      \"client-server\": {\n        \"expiry\": \"8760h\",\n        \"usages\": [ \"key encipherment\", \"server auth\", \"client auth\" ]\n      }\n    }\n  }\n}' | tee ca-config.json\n\necho '{\n  \"CN\": \"etcd-ca\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"US\",\n      \"L\": \"San Jose\",\n      \"O\": \"etcd\",\n      \"OU\": \"CA\",\n      \"ST\": \"California\"\n    }\n  ]\n}' | tee etcd-ca-csr.json\n\ncfssl gencert -initca etcd-ca-csr.json | cfssljson -bare etcd-ca\n\necho '{\n  \"CN\": \"etcd\",\n  \"hosts\": [],\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"US\",\n      \"L\": \"San Jose\",\n      \"O\": \"etcd\",\n      \"OU\": \"CA\",\n      \"ST\": \"California\"\n    }\n  ]\n}' | tee etcd-csr.json\n\ncfssl gencert -ca=etcd-ca.pem -ca-key=etcd-ca-key.pem -config=ca-config.json -hostname=${CONTROLLER_IPS} -profile=server etcd-csr.json | cfssljson -bare etcd-server\ncfssl gencert -ca=etcd-ca.pem -ca-key=etcd-ca-key.pem -config=ca-config.json -hostname=${CONTROLLER_IPS} -profile=client-server etcd-csr.json | cfssljson -bare etcd-peer\ncfssl gencert -ca=etcd-ca.pem -ca-key=etcd-ca-key.pem -config=ca-config.json -hostname=${CONTROLLER_IPS} -profile=client etcd-csr.json | cfssljson -bare etcd-client\n\necho '{\n  \"CN\": \"kube-ca\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"US\",\n      \"L\": \"San Jose\",\n      \"O\": \"kube\",\n      \"OU\": \"CA\",\n      \"ST\": \"California\"\n    }\n  ]\n}' | tee kube-ca-csr.json\n\ncfssl gencert -initca kube-ca-csr.json | cfssljson -bare ca\n\necho '{\n  \"CN\": \"kubernetes\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"US\",\n      \"L\": \"San Jose\",\n      \"O\": \"kube\",\n      \"OU\": \"Cluster\",\n      \"ST\": \"California\"\n    }\n  ]\n}' | tee kubernetes-csr.json\n\ncfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -hostname=${CONTROLLER_NAMES},${CONTROLLER_IPS},${MINION_NAMES},${MINION_IPS},${FIRST_IP_SERVICE_SUBNET},127.0.0.1,kubernetes.default,kubernetes,kubernetes.default.svc,kubernetes.default.svc.cluster.local -profile=server kubernetes-csr.json | cfssljson -bare kubernetes\n\necho '{\n  \"CN\": \"system:kube-controller-manager\",\n  \"hosts\": [],\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"US\",\n      \"L\": \"San Jose\",\n      \"O\": \"system:kube-controller-manager\",\n      \"OU\": \"Cluster\",\n      \"ST\": \"California\"\n    }\n  ]\n}' | tee kube-controller-manager-csr.json\n\ncfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=server kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager\n\necho '{\n  \"CN\": \"system:kube-scheduler\",\n  \"hosts\": [],\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"US\",\n      \"L\": \"San Jose\",\n      \"O\": \"system:kube-scheduler\",\n      \"OU\": \"Cluster\",\n      \"ST\": \"California\"\n    }\n  ]\n}' | tee kube-scheduler-csr.json\n\ncfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=server kube-scheduler-csr.json | cfssljson -bare kube-scheduler\n\ncount=0\nfor ip in $(echo ${CONTROLLER_IPS} | tr \",\" \"\\n\"); do\ninstance=\"controller${count}\"\necho \"{\n  \\\"CN\\\": \\\"system:node:${instance}\\\",\n  \\\"key\\\": {\n    \\\"algo\\\": \\\"rsa\\\",\n    \\\"size\\\": 2048\n  },\n  \\\"names\\\": [\n    {\n      \\\"C\\\": \\\"US\\\",\n      \\\"L\\\": \\\"San Jose\\\",\n      \\\"O\\\": \\\"system:nodes\\\",\n      \\\"OU\\\": \\\"Kubernetes The Hard Way\\\",\n      \\\"ST\\\": \\\"California\\\"\n    }\n  ]\n}\" | tee ${instance}-csr.json\ncfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -hostname=${instance},${ip} -profile=client-server ${instance}-csr.json | cfssljson -bare ${instance}\ncount=$((count+1))\ndone \n\n# -*- Creating kube-proxy certificates\necho '{\n  \"CN\": \"system:kube-proxy\",\n  \"hosts\": [],\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"US\",\n      \"L\": \"San Jose\",\n      \"O\": \"system:node-proxier\",\n      \"OU\": \"Cluster\",\n      \"ST\": \"California\"\n    }\n  ]\n}' | tee kube-proxy-csr.json\n\ncfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=client kube-proxy-csr.json | cfssljson -bare kube-proxy\n\necho '{\n  \"CN\": \"admin\",\n  \"hosts\": [],\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"US\",\n      \"L\": \"San Jose\",\n      \"O\": \"system:masters\",\n      \"OU\": \"Cluster\",\n      \"ST\": \"California\"\n    }\n  ]\n}' | tee admin-csr.json\n\ncfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=client admin-csr.json | cfssljson -bare admin\n\ncount=0\nfor ip in $(echo ${CONTROLLER_IPS} | tr \",\" \"\\n\"); do\nkubectl config set-cluster ${KUBE_CLUSTER_NAME} --certificate-authority=ca.pem --embed-certs=true --server=https:\/\/${INTERNAL_IP}:${MASTER_API_HTTPS} --kubeconfig=controller${count}.kubeconfig\nkubectl config set-credentials system:node:controller${count} --client-certificate=controller${count}.pem --client-key=controller${count}-key.pem --embed-certs=true --kubeconfig=controller${count}.kubeconfig\nkubectl config set-context default --cluster=${KUBE_CLUSTER_NAME} --user=system:node:controller${count} --kubeconfig=controller${count}.kubeconfig\nkubectl config use-context default --kubeconfig=controller${count}.kubeconfig\ncount=$((count+1))\ndone\n\nkubectl config set-cluster ${KUBE_CLUSTER_NAME} --certificate-authority=ca.pem --embed-certs=true --server=https:\/\/${INTERNAL_IP}:${MASTER_API_HTTPS} --kubeconfig=kube-controller-manager.kubeconfig\nkubectl config set-credentials kube-controller-manager --client-certificate=kube-controller-manager.pem --client-key=kube-controller-manager-key.pem --embed-certs=true --kubeconfig=kube-controller-manager.kubeconfig\nkubectl config set-context default --cluster=${KUBE_CLUSTER_NAME} --user=kube-controller-manager --kubeconfig=kube-controller-manager.kubeconfig\nkubectl config use-context default --kubeconfig=kube-controller-manager.kubeconfig\n\nkubectl config set-cluster ${KUBE_CLUSTER_NAME} --certificate-authority=ca.pem --embed-certs=true --server=https:\/\/${INTERNAL_IP}:${MASTER_API_HTTPS} --kubeconfig=kube-scheduler.kubeconfig\nkubectl config set-credentials kube-scheduler --client-certificate=kube-scheduler.pem --client-key=kube-scheduler-key.pem --embed-certs=true --kubeconfig=kube-scheduler.kubeconfig\nkubectl config set-context default --cluster=${KUBE_CLUSTER_NAME} --user=kube-scheduler --kubeconfig=kube-scheduler.kubeconfig\nkubectl config use-context default --kubeconfig=kube-scheduler.kubeconfig\n\nkubectl config set-cluster ${KUBE_CLUSTER_NAME} --certificate-authority=ca.pem --embed-certs=true --server=https:\/\/${INTERNAL_IP}:${MASTER_API_HTTPS} --kubeconfig=kube-proxy.kubeconfig\nkubectl config set-credentials kube-proxy --client-certificate=kube-proxy.pem --client-key=kube-proxy-key.pem --embed-certs=true --kubeconfig=kube-proxy.kubeconfig\nkubectl config set-context default --cluster=${KUBE_CLUSTER_NAME} --user=kube-proxy --kubeconfig=kube-proxy.kubeconfig\nkubectl config use-context default --kubeconfig=kube-proxy.kubeconfig\n\nENCRYPTION_KEY=$(head -c 32 \/dev\/urandom | base64)\necho \"kind: EncryptionConfig\napiVersion: v1\nresources:\n  - resources:\n      - secrets\n    providers:\n      - aescbc:\n          keys:\n            - name: key1\n              secret: ${ENCRYPTION_KEY}\n      - identity: {}\" | tee encryption-config.yaml\n\necho \"@@{CENTOS.secret}@@\" | tee ~\/.ssh\/id_rsa\nchmod 400 ~\/.ssh\/id_rsa\n\ncount=0\nfor ip in $(echo ${CONTROLLER_IPS} | tr \",\" \"\\n\"); do\n  instance=\"controller${count}\"\n  scp -o stricthostkeychecking=no admin*.pem ca*.pem etcd*.pem kubernetes*.pem ${instance}* kube-proxy.kubeconfig kube-controller-manager.kubeconfig kube-scheduler.kubeconfig encryption-config.yaml ${instance}:\ncount=$((count+1))\ndone","script_type":"sh","type":""},"child_tasks_local_reference_list":[],"description":"","message_list":[],"name":"Generate Certs","retries":"0","state":"ACTIVE","target_any_local_reference":{"kind":"app_service","name":"Kubernetes_Master"},"timeout_secs":"0","type":"EXEC","variable_list":[]},{"attrs":{"command_line_args":"","exit_status":[],"script":"#!\/bin\/bash\nset -ex\n\nif [ @@{calm_array_index}@@ -ne 0 ];then\n  exit\nfi\n\nPRISM_CLUSTER_IP=\"@@{PE_CLUSTER_IP}@@\"\nPRISM_DATA_SERVICE_IP=\"@@{PE_CLUSTER_IP}@@\"\n\nif [[ (\"${PRISM_CLUSTER_IP}x\" == \"x\") && (\"${PRISM_DATA_SERVICE_IP}x\" == \"x\") ]]; then exit 0; fi\nexport PATH=$PATH:\/opt\/bin\n\nsudo -E mkdir \/etc\/kubernetes\/addons\/volume_plugin\nNTNX_SECRET=$(echo -n \"@@{PE_USERNAME}@@:@@{PE_PASSWORD}@@\" | base64)\necho 'apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: nutanixabs-provisioner\n  namespace: kube-system\n---\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io\/v1beta1\nmetadata:\n  name: nutanixabs-provisioner-runner\n  namespace: kube-system\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"persistentvolumes\"]\n    verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"]\n  - apiGroups: [\"\"]\n    resources: [\"persistentvolumeclaims\"]\n    verbs: [\"get\", \"list\", \"watch\", \"update\"]\n  - apiGroups: [\"storage.k8s.io\"]\n    resources: [\"storageclasses\"]\n    verbs: [\"get\", \"list\", \"watch\"]\n  - apiGroups: [\"\"]\n    resources: [\"events\"]\n    verbs: [\"list\", \"watch\", \"create\", \"update\", \"patch\"]\n  - apiGroups: [\"\"]\n    resources: [\"services\"]\n    verbs: [\"get\"]\n  - apiGroups: [\"\"]\n    resources: [\"secrets\"]\n    verbs: [\"get\", \"create\", \"delete\"]\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io\/v1beta1\nmetadata:\n  name: run-nutanixabs-provisioner\n  namespace: kube-system\nsubjects:\n  - kind: ServiceAccount\n    name: nutanixabs-provisioner\n    namespace: kube-system\nroleRef:\n  kind: ClusterRole\n  name: nutanixabs-provisioner-runner\n  apiGroup: rbac.authorization.k8s.io\n---\napiVersion: extensions\/v1beta1\nkind: Deployment\nmetadata:\n  name: nutanixabs-provisioner\n  namespace: kube-system\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: nutanixabs-provisioner\n    spec:\n      serviceAccount: nutanixabs-provisioner\n      hostNetwork: true\n      containers:\n        -\n          image: \"ntnx\/nutanixabs-provisioner\"\n          imagePullPolicy: IfNotPresent\n          name: nutanixabs-provisioner\n          args: [\"--v=4\"]\n          imagePullPolicy: IfNotPresent\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: ntnx-secret\n  namespace: kube-system\ndata:\n  key: <SECRET>\ntype: nutanix\/abs\n---\nkind: StorageClass\napiVersion: storage.k8s.io\/v1\nmetadata:\n  name: silver \n  namespace: kube-system\n  annotations:\n    storageclass.beta.kubernetes.io\/is-default-class: \"true\"\nprovisioner: nutanix\/abs\nparameters:\n     prismEndPoint: @@{PE_CLUSTER_IP}@@:9440\n     dataServiceEndPoint: @@{PE_DATA_SERVICE_IP}@@:3260\n     secretName: ntnx-secret\n     storageContainer: @@{PE_CONTAINER_NAME}@@\n     fsType: ext4\n     chapAuthEnabled: \"false\"' | sudo -E tee \/etc\/kubernetes\/addons\/volume_plugin\/nutanix-provisioner.yaml\n\nsudo -E sed -i \"s\/<SECRET>\/${NTNX_SECRET}\/\" \/etc\/kubernetes\/addons\/volume_plugin\/nutanix-provisioner.yaml\nkubectl create -f \/etc\/kubernetes\/addons\/volume_plugin\/nutanix-provisioner.yaml\n\necho \"apiVersion: v1\nkind: Secret\nmetadata:\n  name: ntnx-secret\n  namespace: default\ndata:\n  key: <SECRET>\ntype: nutanix\/abs\" | sudo -E tee -a \/etc\/kubernetes\/addons\/volume_plugin\/ntnx-secret.yaml\nsudo -E sed -i \"s\/<SECRET>\/${NTNX_SECRET}\/\" \/etc\/kubernetes\/addons\/volume_plugin\/ntnx-secret.yaml\nkubectl create -f \/etc\/kubernetes\/addons\/volume_plugin\/ntnx-secret.yaml\n","script_type":"sh","type":""},"child_tasks_local_reference_list":[],"description":"","message_list":[],"name":"NVP Configuration","retries":"0","state":"ACTIVE","target_any_local_reference":{"kind":"app_service","name":"Kubernetes_Master"},"timeout_secs":"0","type":"EXEC","variable_list":[]},{"attrs":{"command_line_args":"","exit_status":[],"script":"#!\/bin\/bash\nset -ex\n\nif [ @@{calm_array_index}@@ -ne 0 ];then\n\texit\nfi\n\nexport PATH=$PATH:\/opt\/bin\nsudo -E mkdir -p \/etc\/kubernetes\/addons\/flannel\necho '---\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io\/v1beta1\nmetadata:\n  name: flannel\nrules:\n  - apiGroups:\n      - \"\"\n    resources:\n      - pods\n    verbs:\n      - get\n  - apiGroups:\n      - \"\"\n    resources:\n      - nodes\n    verbs:\n      - list\n      - watch\n  - apiGroups:\n      - \"\"\n    resources:\n      - nodes\/status\n    verbs:\n      - patch\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io\/v1beta1\nmetadata:\n  name: flannel\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: flannel\nsubjects:\n- kind: ServiceAccount\n  name: flannel\n  namespace: kube-system\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: flannel\n  namespace: kube-system\n---\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: kube-flannel-cfg\n  namespace: kube-system\n  labels:\n    tier: node\n    app: flannel\ndata:\n  cni-conf.json: |\n    {\n      \"name\": \"cbr0\",\n      \"type\": \"flannel\",\n      \"delegate\": {\n        \"isDefaultGateway\": true\n      }\n    }\n  net-conf.json: |\n    {\n      \"Network\": \"@@{KUBE_CLUSTER_SUBNET}@@\",\n      \"Backend\": {\n        \"Type\": \"vxlan\"\n      }\n    }\n---\napiVersion: apps\/v1beta2 #extensions\/v1beta1\nkind: DaemonSet\nmetadata:\n  name: kube-flannel-ds\n  namespace: kube-system\n  labels:\n    tier: node\n    app: flannel\nspec:\n  selector:\n    matchLabels:\n      app: flannel\n  template:\n    metadata:\n      labels:\n        tier: node\n        app: flannel\n    spec:\n      hostNetwork: true\n      nodeSelector:\n        beta.kubernetes.io\/arch: amd64\n      tolerations:\n      - key: node-role.kubernetes.io\/master\n        operator: Exists\n        effect: NoSchedule\n      serviceAccountName: flannel\n      initContainers:\n      - name: install-cni\n        image: quay.io\/coreos\/flannel:v0.10.0-amd64\n        command:\n        - cp\n        args:\n        - -f\n        - \/etc\/kube-flannel\/cni-conf.json\n        - \/etc\/cni\/net.d\/10-flannel.conf\n        volumeMounts:\n        - name: cni\n          mountPath: \/etc\/cni\/net.d\n        - name: flannel-cfg\n          mountPath: \/etc\/kube-flannel\/\n      containers:\n      - name: kube-flannel\n        image: quay.io\/coreos\/flannel:v0.10.0-amd64\n        imagePullPolicy: IfNotPresent\n        command: [ \"\/opt\/bin\/flanneld\", \"--ip-masq\", \"--kube-subnet-mgr\" ]\n        securityContext:\n          privileged: true\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: run\n          mountPath: \/run\n        - name: flannel-cfg\n          mountPath: \/etc\/kube-flannel\/\n      volumes:\n        - name: run\n          hostPath:\n            path: \/run\n        - name: cni\n          hostPath:\n            path: \/etc\/cni\/net.d\n        - name: flannel-cfg\n          configMap:\n            name: kube-flannel-cfg' | sudo -E tee \/etc\/kubernetes\/addons\/flannel\/kube-flannel.yml\nkubectl create -f \/etc\/kubernetes\/addons\/flannel\/kube-flannel.yml\nsleep 15\n","script_type":"sh","type":""},"child_tasks_local_reference_list":[],"description":"","message_list":[],"name":"Network Configuration","retries":"0","state":"ACTIVE","target_any_local_reference":{"kind":"app_service","name":"Kubernetes_Master"},"timeout_secs":"0","type":"EXEC","variable_list":[]},{"attrs":{"edges":[{"edge_type":"user_defined","from_task_reference":{"kind":"app_task","name":"Add User Roles"},"to_task_reference":{"kind":"app_task","name":"Network Configuration"},"type":""},{"edge_type":"user_defined","from_task_reference":{"kind":"app_task","name":"Configure Services"},"to_task_reference":{"kind":"app_task","name":"Add User Roles"},"type":""},{"edge_type":"user_defined","from_task_reference":{"kind":"app_task","name":"DNS Configuration"},"to_task_reference":{"kind":"app_task","name":"NVP Configuration"},"type":""},{"edge_type":"user_defined","from_task_reference":{"kind":"app_task","name":"ETCD Docker Kubelet Install"},"to_task_reference":{"kind":"app_task","name":"Generate Certs"},"type":""},{"edge_type":"user_defined","from_task_reference":{"kind":"app_task","name":"Generate Certs"},"to_task_reference":{"kind":"app_task","name":"Configure Services"},"type":""},{"edge_type":"user_defined","from_task_reference":{"kind":"app_task","name":"Network Configuration"},"to_task_reference":{"kind":"app_task","name":"DNS Configuration"},"type":""}],"type":""},"child_tasks_local_reference_list":[{"kind":"app_task","name":"Add User Roles"},{"kind":"app_task","name":"Configure Services"},{"kind":"app_task","name":"DNS Configuration"},{"kind":"app_task","name":"ETCD Docker Kubelet Install"},{"kind":"app_task","name":"Generate Certs"},{"kind":"app_task","name":"NVP Configuration"},{"kind":"app_task","name":"Network Configuration"}],"description":"","message_list":[],"name":"dac441af_dag","retries":"0","state":"ACTIVE","target_any_local_reference":{"kind":"app_package","name":"AHV_Centos_K8SC_Package"},"timeout_secs":"0","type":"DAG","variable_list":[]}],"variable_list":[]},"type":"","uninstall_runbook":{"description":"","main_task_local_reference":{"kind":"app_task","name":"6324786a_dag"},"message_list":[],"name":"6522ac7a_runbook","state":"ACTIVE","task_definition_list":[{"attrs":{"edges":[],"type":""},"child_tasks_local_reference_list":[],"description":"","message_list":[],"name":"6324786a_dag","retries":"0","state":"ACTIVE","target_any_local_reference":{"kind":"app_package","name":"AHV_Centos_K8SC_Package"},"timeout_secs":"0","type":"DAG","variable_list":[]}],"variable_list":[]}},"service_local_reference_list":[{"kind":"app_service","name":"Kubernetes_Master"}],"type":"DEB","variable_list":[],"version":""},{"action_list":[],"description":"","name":"AHV_Centos_K8SM_Package","options":{"install_runbook":{"description":"","main_task_local_reference":{"kind":"app_task","name":"c1566b3d_dag"},"message_list":[],"name":"9a6e189b_runbook","state":"ACTIVE","task_definition_list":[{"attrs":{"command_line_args":"","exit_status":[],"script":"#!\/bin\/bash\nset -ex\n\n\n\nKUBELET_IMAGE_TAG=\"@@{KUBE_IMAGE_TAG}@@\"\nif [[ \"@@{KUBE_IMAGE_TAG_NEW}@@x\" != \"x\" ]]; then\n\tKUBELET_IMAGE_TAG=\"@@{KUBE_IMAGE_TAG_NEW}@@\"\nfi\nINTERNAL_IP=\"@@{address}@@\"\nCONTROLLER_IPS=\"@@{AHV_Centos_K8SC.address}@@\"\nNODE_NAME=\"minion@@{calm_array_index}@@\"\nCLUSTER_SUBNET=\"@@{KUBE_CLUSTER_SUBNET}@@\"\nSERVICE_SUBNET=\"@@{KUBE_SERVICE_SUBNET}@@\"\nKUBE_CLUSTER_DNS=\"@@{KUBE_DNS_IP}@@\"\nDOCKER_VERSION=\"@@{DOCKER_VERSION}@@\"\nKUBE_CERT_PATH=\"\/etc\/kubernetes\/ssl\"\nKUBE_MANIFEST_PATH=\"\/etc\/kubernetes\/manifests\"\nKUBE_CNI_BIN_PATH=\"\/opt\/cni\/bin\"\nKUBE_CNI_CONF_PATH=\"\/etc\/cni\/net.d\"\nETCD_SERVER_PORT=2379\nVERSION=$(echo \"${KUBELET_IMAGE_TAG}\" | tr \"_\" \" \" | awk '{print $1}')\n\nsudo -E mkdir -p ${KUBE_CERT_PATH} ${KUBE_MANIFEST_PATH} ${KUBE_CNI_CONF_PATH} ${KUBE_CNI_BIN_PATH}\nsudo -E hostnamectl set-hostname --static ${NODE_NAME}\n\n#sudo -E yum update -y --quiet\nsudo -E yum install -y wget iscsi-initiator-utils socat --quiet\n\n#wget -q https:\/\/github.com\/containernetworking\/plugins\/releases\/download\/v0.6.0\/cni-plugins-amd64-v0.6.0.tgz\n#wget -q https:\/\/storage.googleapis.com\/kubernetes-release\/release\/${VERSION}\/bin\/linux\/amd64\/kubelet\n#wget -q https:\/\/storage.googleapis.com\/kubernetes-release\/release\/${VERSION}\/bin\/linux\/amd64\/kubectl\n#wget -q https:\/\/pkg.cfssl.org\/R1.2\/cfssl_linux-amd64 https:\/\/pkg.cfssl.org\/R1.2\/cfssljson_linux-amd64\n\nchmod +x kubelet kubectl cfssl_linux-amd64 cfssljson_linux-amd64\nsudo -E mv kubelet kubectl \/usr\/bin\/\nsudo -E mv cfssl_linux-amd64 \/usr\/local\/bin\/cfssl\nsudo -E mv cfssljson_linux-amd64 \/usr\/local\/bin\/cfssljson\n\nsudo -E yum install -y --quiet yum-utils\n#sudo -E yum-config-manager --add-repo https:\/\/download.docker.com\/linux\/centos\/docker-ce.repo\n#sudo -E yum install -y --quiet --setopt=obsoletes=0 docker-ce-${DOCKER_VERSION} docker-ce-selinux-${DOCKER_VERSION}\n\n#sudo -E sed -i '\/ExecStart=\/c\\\\ExecStart=\/usr\/bin\/dockerd -H tcp:\/\/0.0.0.0:2375 -H unix:\/\/\/var\/run\/docker.sock' \/usr\/lib\/systemd\/system\/docker.service\n\n#cp \/usr\/lib\/systemd\/system\/docker.service \/tmp\n#sudo -E sed -i '\/\\[Service\\]\/c\\\\[Service]\\nEnvironment=\\\"HTTP_PROXY=http:\/\/192.168.0.209:1080\/\\\"' \/usr\/lib\/systemd\/system\/docker.service\n\nsudo -E systemctl enable docker\nsudo -E usermod -a -G docker $USER\n\n#sudo -E mkdir -p \/etc\/docker\n#echo '{\n#  \"storage-driver\": \"overlay\"\n#}' | sudo -E tee \/etc\/docker\/daemon.json\n\necho '{\n  \"name\": \"cbr0\",\n  \"type\": \"flannel\",\n  \"delegate\": {\n    \"isDefaultGateway\": true\n  }\n}' | sudo -E tee ${KUBE_CNI_CONF_PATH}\/10-flannel.conf\n\nsudo -E tar -zxvf cni-plugins-amd64-v0.6.0.tgz -C ${KUBE_CNI_BIN_PATH}\nrm -rf cni-plugins-amd64-v0.6.0.tgz\n\necho \"[Unit]\nDescription=Kubernetes Kubelet\nDocumentation=https:\/\/github.com\/GoogleCloudPlatform\/kubernetes\nAfter=docker.service\nRequires=docker.service\n\n[Service]\nExecStart=\/usr\/bin\/kubelet \\\\\n  --allow-privileged=true \\\\\n  --anonymous-auth=false \\\\\n  --authorization-mode=Webhook \\\\\n  --cluster-dns=${KUBE_CLUSTER_DNS} \\\\\n  --cluster-domain=cluster.local \\\\\n  --container-runtime=docker \\\\\n  --enable-custom-metrics \\\\\n  --kubeconfig=${KUBE_CERT_PATH}\/${NODE_NAME}.kubeconfig \\\\\n  --network-plugin=cni \\\\\n  --pod-cidr=${CLUSTER_SUBNET} \\\\\n  --register-node=true \\\\\n  --runtime-request-timeout=10m \\\\\n  --client-ca-file=${KUBE_CERT_PATH}\/ca.pem \\\\\n  --tls-cert-file=${KUBE_CERT_PATH}\/${NODE_NAME}.pem \\\\\n  --tls-private-key-file=${KUBE_CERT_PATH}\/${NODE_NAME}-key.pem \\\\\n  --pod-manifest-path=${KUBE_MANIFEST_PATH} \\\\\n  --read-only-port=0 \\\\\n  --protect-kernel-defaults=false \\\\\n  --make-iptables-util-chains=true \\\\\n  --keep-terminated-pod-volumes=false \\\\\n  --event-qps=0 \\\\\n  --cadvisor-port=0 \\\\\n  --runtime-cgroups=\/systemd\/system.slice \\\\\n  --kubelet-cgroups=\/systemd\/system.slice \\\\\n  --eviction-hard=memory.available<100Mi,nodefs.available<10%,nodefs.inodesFree<5% \\\\\n  --node-labels 'node-role.kubernetes.io\/worker=true' \\\\\n  --node-labels 'beta.kubernetes.io\/fluentd-ds-ready=true' \\\\\n  --v=2\nRestart=on-failure\nRestartSec=5\n\n[Install]\nWantedBy=multi-user.target\" | sudo -E tee \/etc\/systemd\/system\/kubelet.service\n\necho \"apiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-proxy\n  namespace: kube-system\nspec:\n  hostNetwork: true\n  containers:\n  - name: kube-proxy\n    image: quay.io\/coreos\/hyperkube:${KUBELET_IMAGE_TAG}\n    imagePullPolicy: IfNotPresent\n    command:\n    - \/hyperkube\n    - proxy\n    - --cluster-cidr=${CLUSTER_SUBNET}\n    - --masquerade-all=true\n    - --kubeconfig=${KUBE_CERT_PATH}\/kube-proxy.kubeconfig\n    - --proxy-mode=iptables\n    securityContext:\n      privileged: true\n    volumeMounts:\n    - mountPath: ${KUBE_CERT_PATH}\n      name: ssl-certs-kubernetes\n      readOnly: true\n    - mountPath: \/etc\/ssl\/certs\n      name: ssl-certs-host\n      readOnly: true\n  volumes:\n  - hostPath:\n      path: ${KUBE_CERT_PATH}\n    name: ssl-certs-kubernetes\n  - hostPath:\n      path: \/usr\/share\/ca-certificates\n    name: ssl-certs-host\" | sudo -E tee ${KUBE_MANIFEST_PATH}\/kube-proxy.yaml\n\necho \"InitiatorName=iqn.1994-05.com.nutanix:k8s-worker\" | sudo -E tee \/etc\/iscsi\/initiatorname.iscsi\n\n#sudo -E mkdir -p \/var\/lib\/docker\n#sudo -E yum install -y lvm2 --quiet\n#sudo -E pvcreate \/dev\/sd{b,c,d}\n#sudo -E vgcreate docker \/dev\/sd{b,c,d}\n\n#sleep 10\n#sudo -E udevadm control --reload-rules\n\n#sudo -E lvcreate -l 100%VG -n docker_lvm docker\n#sudo -E mkfs.xfs \/dev\/docker\/docker_lvm\n\n#echo -e \"\/dev\/docker\/docker_lvm \\t \/var\/lib\/docker \\t xfs \\t defaults \\t 0 0\" | sudo -E tee -a \/etc\/fstab\n#sudo -E mount -a\n\necho 'exclude=docker*' | sudo -E tee -a \/etc\/yum.conf\n\necho \"@@{CENTOS.secret}@@\" | tee ~\/.ssh\/id_rsa\nchmod 400 ~\/.ssh\/id_rsa\n\n#while [ ! -f ${NODE_NAME}.kubeconfig ] ; do  echo \"waiting for certs sleeping 5\" && sleep 5; done\n\n#sudo -E cp *.pem *.kubeconfig ${KUBE_CERT_PATH}\/\n#sudo -E chmod +r ${KUBE_CERT_PATH}\/*\n\n#sudo -E systemctl start docker kubelet iscsid\n#sudo -E systemctl enable docker kubelet iscsid","script_type":"sh","type":""},"child_tasks_local_reference_list":[],"description":"","message_list":[],"name":"Docker Kubelet Install","retries":"0","state":"ACTIVE","target_any_local_reference":{"kind":"app_service","name":"Kubernetes_Minion"},"timeout_secs":"0","type":"EXEC","variable_list":[]},{"attrs":{"command_line_args":"","exit_status":[],"script":"#!\/bin\/bash\nset -ex\n\nKUBE_CLUSTER_NAME=\"@@{KUBE_CLUSTER_NAME}@@\"\nMASTER_IP=\"@@{AHV_Centos_K8SC.address[0]}@@\"\nINSTANCE_IP=\"@@{address}@@\"\ninstance=\"minion@@{calm_array_index}@@\"\nKUBE_CERT_PATH=\"\/etc\/kubernetes\/ssl\"\nMASTER_API_HTTPS=6443\n\nwhile [ ! $(ssh -o stricthostkeychecking=no $MASTER_IP \"ls \/opt\/kube-ssl\/encryption-config.yaml 2>\/dev\/null\") ] ; do  echo \"waiting for certs sleeping 5\" && sleep 5; done\n\nscp -o stricthostkeychecking=no ${MASTER_IP}:\/opt\/kube-ssl\/{ca*.pem,kubernetes*.pem,kube-proxy.kubeconfig,ca-config.json} .\n\ninstance=\"minion@@{calm_array_index}@@\"\necho \"{\n  \\\"CN\\\": \\\"system:node:${instance}\\\",\n  \\\"key\\\": {\n    \\\"algo\\\": \\\"rsa\\\",\n    \\\"size\\\": 2048\n  },\n  \\\"names\\\": [\n    {\n      \\\"C\\\": \\\"US\\\",\n      \\\"L\\\": \\\"San Jose\\\",\n      \\\"O\\\": \\\"system:nodes\\\",\n      \\\"OU\\\": \\\"Kubernetes The Hard Way\\\",\n      \\\"ST\\\": \\\"California\\\"\n    }\n  ]\n}\" | tee ${instance}-csr.json\ncfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -hostname=${instance},${INSTANCE_IP} -profile=client-server ${instance}-csr.json | cfssljson -bare ${instance}\n\nkubectl config set-cluster ${KUBE_CLUSTER_NAME} --certificate-authority=ca.pem --embed-certs=true --server=https:\/\/${MASTER_IP}:${MASTER_API_HTTPS} --kubeconfig=${instance}.kubeconfig\nkubectl config set-credentials system:node:${instance} --client-certificate=${instance}.pem --client-key=${instance}-key.pem --embed-certs=true --kubeconfig=${instance}.kubeconfig\nkubectl config set-context default --cluster=${KUBE_CLUSTER_NAME} --user=system:node:${instance} --kubeconfig=${instance}.kubeconfig\nkubectl config use-context default --kubeconfig=${instance}.kubeconfig\n\nsudo -E cp *.pem *.kubeconfig ${KUBE_CERT_PATH}\/\nsudo -E chmod +r ${KUBE_CERT_PATH}\/*\n\nrm -rf ${instance}-csr.json","script_type":"sh","type":""},"child_tasks_local_reference_list":[],"description":"","message_list":[],"name":"GetCerts","retries":"0","state":"ACTIVE","target_any_local_reference":{"kind":"app_service","name":"Kubernetes_Minion"},"timeout_secs":"0","type":"EXEC","variable_list":[]},{"attrs":{"edges":[{"edge_type":"user_defined","from_task_reference":{"kind":"app_task","name":"Docker Kubelet Install"},"to_task_reference":{"kind":"app_task","name":"GetCerts"},"type":""}],"type":""},"child_tasks_local_reference_list":[{"kind":"app_task","name":"Docker Kubelet Install"},{"kind":"app_task","name":"GetCerts"}],"description":"","message_list":[],"name":"c1566b3d_dag","retries":"0","state":"ACTIVE","target_any_local_reference":{"kind":"app_package","name":"AHV_Centos_K8SM_Package"},"timeout_secs":"0","type":"DAG","variable_list":[]}],"variable_list":[]},"type":"","uninstall_runbook":{"description":"","main_task_local_reference":{"kind":"app_task","name":"abd3357c_dag"},"message_list":[],"name":"3cb17acf_runbook","state":"ACTIVE","task_definition_list":[{"attrs":{"command_line_args":"","exit_status":[],"script":"#!\/bin\/bash\nset -ex\n\nMASTER_IP=\"@@{Kubernetes_Master.address[0]}@@\"\nssh -o stricthostkeychecking=no ${MASTER_IP} \"kubectl drain 'minion@@{calm_array_index}@@' --ignore-daemonsets --delete-local-data --force\"\nsleep 10\nssh -o stricthostkeychecking=no ${MASTER_IP} \"kubectl delete node 'minion@@{calm_array_index}@@'\"","script_type":"sh","type":""},"child_tasks_local_reference_list":[],"description":"","message_list":[],"name":"Remove Node","retries":"0","state":"ACTIVE","target_any_local_reference":{"kind":"app_service","name":"Kubernetes_Minion"},"timeout_secs":"0","type":"EXEC","variable_list":[]},{"attrs":{"edges":[],"type":""},"child_tasks_local_reference_list":[{"kind":"app_task","name":"Remove Node"}],"description":"","message_list":[],"name":"abd3357c_dag","retries":"0","state":"ACTIVE","target_any_local_reference":{"kind":"app_package","name":"AHV_Centos_K8SM_Package"},"timeout_secs":"0","type":"DAG","variable_list":[]}],"variable_list":[]}},"service_local_reference_list":[{"kind":"app_service","name":"Kubernetes_Minion"}],"type":"DEB","variable_list":[],"version":""}],"published_service_definition_list":[],"service_definition_list":[{"action_list":[{"critical":false,"description":"System action for creating an application","name":"action_create","runbook":{"description":"","main_task_local_reference":{"kind":"app_task","name":"f33eac3f_dag"},"name":"a9639da1_runbook","task_definition_list":[{"attrs":{"command_line_args":"","exit_status":[],"script":"#!\/bin\/bash\nset -ex\n\nif [ @@{calm_array_index}@@ -ne 0 ];then\n\texit\nfi\nexport PATH=$PATH:\/opt\/bin\n\nsudo -E mkdir \/etc\/kubernetes\/addons\/dashboard\necho '# Copyright 2015 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Configuration to deploy release version of the Dashboard UI compatible with\n# Kubernetes 1.7.\n#\n# Example usage: kubectl create -f <this_file>\n\n# ------------------- Dashboard Secret ------------------- #\n\napiVersion: v1\nkind: Secret\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: kubernetes-dashboard-certs\n  namespace: kube-system\ntype: Opaque\n\n---\n# ------------------- Dashboard Service Account ------------------- #\n\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: kubernetes-dashboard\n  namespace: kube-system\n\n---\n# ------------------- Dashboard Role & Role Binding ------------------- #\n\n---\napiVersion: rbac.authorization.k8s.io\/v1beta1\nkind: ClusterRoleBinding\nmetadata:\n  name: kubernetes-full-access\n  namespace: kube-system\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- kind: ServiceAccount\n  name: kubernetes-dashboard\n  namespace: kube-system\n\n---\n# ------------------- Dashboard Deployment ------------------- #\n\nkind: Deployment\napiVersion: apps\/v1beta2\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: kubernetes-dashboard\n  namespace: kube-system\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      k8s-app: kubernetes-dashboard\n  template:\n    metadata:\n      labels:\n        k8s-app: kubernetes-dashboard\n    spec:\n      containers:\n      - name: kubernetes-dashboard\n        image: k8s.gcr.io\/kubernetes-dashboard-amd64:v1.8.3\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8443\n          protocol: TCP\n        args:\n          - --auto-generate-certificates\n          # Uncomment the following line to manually specify Kubernetes API server Host\n          # If not specified, Dashboard will attempt to auto discover the API server and connect\n          # to it. Uncomment only if the default does not work.\n          # - --apiserver-host=http:\/\/my-address:port\n        volumeMounts:\n        - name: kubernetes-dashboard-certs\n          mountPath: \/certs\n          # Create on-disk volume to store exec logs\n        - mountPath: \/tmp\n          name: tmp-volume\n        livenessProbe:\n          httpGet:\n            scheme: HTTPS\n            path: \/\n            port: 8443\n          initialDelaySeconds: 30\n          timeoutSeconds: 30\n      volumes:\n      - name: kubernetes-dashboard-certs\n        secret:\n          secretName: kubernetes-dashboard-certs\n      - name: tmp-volume\n        emptyDir: {}\n      serviceAccountName: kubernetes-dashboard\n      # Comment the following tolerations if Dashboard must not be deployed on master\n      tolerations:\n      - key: node-role.kubernetes.io\/master\n        effect: NoSchedule\n\n---\n# ------------------- Dashboard Service ------------------- #\n\nkind: Service\napiVersion: v1\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: kubernetes-dashboard\n  namespace: kube-system\nspec:\n  type: NodePort\n  ports:\n    - port: 8443\n      nodePort: 30443\n  selector:\n    k8s-app: kubernetes-dashboard' | sudo -E tee \/etc\/kubernetes\/addons\/dashboard\/kubernetes-dashboard.yaml\nkubectl create -f \/etc\/kubernetes\/addons\/dashboard\/kubernetes-dashboard.yaml\n","script_type":"sh","type":""},"child_tasks_local_reference_list":[],"description":"","name":"Dashboard","retries":"0","target_any_local_reference":{"kind":"app_service","name":"Kubernetes_Master"},"timeout_secs":"0","type":"EXEC","variable_list":[]},{"attrs":{"command_line_args":"","exit_status":[],"script":"#!\/bin\/bash\nset -x\n\nif [ @@{calm_array_index}@@ -ne 0 ];then\n\texit\nfi\nexport PATH=$PATH:\/opt\/bin\nsudo -E mkdir \/etc\/kubernetes\/addons\/helm\necho \"#helm init --service-account helm\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: helm\n  namespace: kube-system\n---\napiVersion: rbac.authorization.k8s.io\/v1beta1\nkind: ClusterRoleBinding\nmetadata:\n  name: helm\n  namespace: kube-system\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- kind: ServiceAccount\n  name: helm\n  namespace: kube-system\" | sudo -E tee \/etc\/kubernetes\/addons\/helm\/helm.yaml\n\nkubectl create -f \/etc\/kubernetes\/addons\/helm\/helm.yaml\nprintf -v no_proxy '%s,' 10.132.129.{1..255}\nexport no_proxy=${no_proxy}localhost\necho $no_proxy\nhttp_proxy=http:\/\/10.132.71.38:1080\/ https_proxy=http:\/\/10.132.71.38:1080\/ no_proxy=${no_proxy} helm init --service-account helm\n\nexit 0\n\n","script_type":"sh","type":""},"child_tasks_local_reference_list":[],"description":"","name":"HELM","retries":"0","target_any_local_reference":{"kind":"app_service","name":"Kubernetes_Master"},"timeout_secs":"0","type":"EXEC","variable_list":[]},{"attrs":{"edges":[{"edge_type":"user_defined","from_task_reference":{"kind":"app_task","name":"Dashboard"},"to_task_reference":{"kind":"app_task","name":"HELM"},"type":""}],"type":""},"child_tasks_local_reference_list":[{"kind":"app_task","name":"Dashboard"},{"kind":"app_task","name":"HELM"}],"description":"","name":"f33eac3f_dag","retries":"0","target_any_local_reference":{"kind":"app_service","name":"Kubernetes_Master"},"timeout_secs":"0","type":"DAG","variable_list":[]}],"variable_list":[]},"type":"system"},{"critical":false,"description":"System action for deleting an application. Deletes created VMs as well","name":"action_delete","runbook":{"description":"","main_task_local_reference":{"kind":"app_task","name":"62a92d34_dag"},"name":"8c4aa276_runbook","task_definition_list":[{"attrs":{"edges":[],"type":""},"child_tasks_local_reference_list":[],"description":"","name":"62a92d34_dag","retries":"0","target_any_local_reference":{"kind":"app_service","name":"Kubernetes_Master"},"timeout_secs":"0","type":"DAG","variable_list":[]}],"variable_list":[]},"type":"system"},{"critical":false,"description":"System action for restarting an application","name":"action_restart","runbook":{"description":"","main_task_local_reference":{"kind":"app_task","name":"414912df_dag"},"name":"d5c8316f_runbook","task_definition_list":[{"attrs":{"edges":[],"type":""},"child_tasks_local_reference_list":[],"description":"","name":"414912df_dag","retries":"0","target_any_local_reference":{"kind":"app_service","name":"Kubernetes_Master"},"timeout_secs":"0","type":"DAG","variable_list":[]}],"variable_list":[]},"type":"system"},{"critical":false,"description":"System action for starting an application","name":"action_start","runbook":{"description":"","main_task_local_reference":{"kind":"app_task","name":"334b4484_dag"},"name":"daee30ae_runbook","task_definition_list":[{"attrs":{"edges":[{"edge_type":"user_defined","from_task_reference":{"kind":"app_task","name":"Start"},"to_task_reference":{"kind":"app_task","name":"GetVariable"},"type":""}],"type":""},"child_tasks_local_reference_list":[{"kind":"app_task","name":"GetVariable"},{"kind":"app_task","name":"Start"}],"description":"","name":"334b4484_dag","retries":"0","target_any_local_reference":{"kind":"app_service","name":"Kubernetes_Master"},"timeout_secs":"0","type":"DAG","variable_list":[]},{"attrs":{"eval_scope":"local","eval_variables":["admin_certificate","admin_key"],"exit_status":[],"script":"admin_certificate=`cat ~\/CA\/admin.pem`\necho \"admin_certificate=${admin_certificate}\"\nadmin_key=`cat ~\/CA\/admin-key.pem`\necho \"admin_key=${admin_key}\"","script_type":"sh","type":""},"child_tasks_local_reference_list":[],"description":"","name":"GetVariable","retries":"0","target_any_local_reference":{"kind":"app_service","name":"Kubernetes_Master"},"timeout_secs":"0","type":"SET_VARIABLE","variable_list":[]},{"attrs":{"command_line_args":"","exit_status":[],"script":"#!\/bin\/bash\nset -ex\nsudo systemctl start etcd docker kubelet\nsudo systemctl enable etcd docker kubelet\n","script_type":"sh","type":""},"child_tasks_local_reference_list":[],"description":"","name":"Start","retries":"0","target_any_local_reference":{"kind":"app_service","name":"Kubernetes_Master"},"timeout_secs":"0","type":"EXEC","variable_list":[]}],"variable_list":[]},"type":"system"},{"critical":false,"description":"System action for stopping an application","name":"action_stop","runbook":{"description":"","main_task_local_reference":{"kind":"app_task","name":"9713c3d0_dag"},"name":"6407a345_runbook","task_definition_list":[{"attrs":{"edges":[],"type":""},"child_tasks_local_reference_list":[],"description":"","name":"9713c3d0_dag","retries":"0","target_any_local_reference":{"kind":"app_service","name":"Kubernetes_Master"},"timeout_secs":"0","type":"DAG","variable_list":[]}],"variable_list":[]},"type":"system"}],"depends_on_list":[],"description":"","name":"Kubernetes_Master","port_list":[],"singleton":false,"tier":"","variable_list":[{"attrs":{"type":""},"description":"","label":"","name":"admin_certificate","type":"LOCAL","val_type":"STRING","value":""},{"attrs":{"type":""},"description":"","label":"","name":"admin_key","type":"LOCAL","val_type":"STRING","value":""}]},{"action_list":[{"critical":false,"description":"System action for creating an application","name":"action_create","runbook":{"description":"","main_task_local_reference":{"kind":"app_task","name":"4af7c45f_dag"},"name":"7c6e4932_runbook","task_definition_list":[{"attrs":{"edges":[],"type":""},"child_tasks_local_reference_list":[],"description":"","name":"4af7c45f_dag","retries":"0","target_any_local_reference":{"kind":"app_service","name":"Kubernetes_Minion"},"timeout_secs":"0","type":"DAG","variable_list":[]}],"variable_list":[]},"type":"system"},{"critical":false,"description":"System action for deleting an application. Deletes created VMs as well","name":"action_delete","runbook":{"description":"","main_task_local_reference":{"kind":"app_task","name":"1e2c50bb_dag"},"name":"1d794193_runbook","task_definition_list":[{"attrs":{"edges":[],"type":""},"child_tasks_local_reference_list":[],"description":"","name":"1e2c50bb_dag","retries":"0","target_any_local_reference":{"kind":"app_service","name":"Kubernetes_Minion"},"timeout_secs":"0","type":"DAG","variable_list":[]}],"variable_list":[]},"type":"system"},{"critical":false,"description":"System action for restarting an application","name":"action_restart","runbook":{"description":"","main_task_local_reference":{"kind":"app_task","name":"1d66b5be_dag"},"name":"3a6f587d_runbook","task_definition_list":[{"attrs":{"edges":[],"type":""},"child_tasks_local_reference_list":[],"description":"","name":"1d66b5be_dag","retries":"0","target_any_local_reference":{"kind":"app_service","name":"Kubernetes_Minion"},"timeout_secs":"0","type":"DAG","variable_list":[]}],"variable_list":[]},"type":"system"},{"critical":false,"description":"System action for starting an application","name":"action_start","runbook":{"description":"","main_task_local_reference":{"kind":"app_task","name":"970128c7_dag"},"name":"884352cf_runbook","task_definition_list":[{"attrs":{"edges":[],"type":""},"child_tasks_local_reference_list":[{"kind":"app_task","name":"Start"}],"description":"","name":"970128c7_dag","retries":"0","target_any_local_reference":{"kind":"app_service","name":"Kubernetes_Minion"},"timeout_secs":"0","type":"DAG","variable_list":[]},{"attrs":{"command_line_args":"","exit_status":[],"script":"#!\/bin\/bash\n#set -ex\n\nsudo systemctl start docker kubelet\nsudo systemctl enable docker kubelet","script_type":"sh","type":""},"child_tasks_local_reference_list":[],"description":"","name":"Start","retries":"0","target_any_local_reference":{"kind":"app_service","name":"Kubernetes_Minion"},"timeout_secs":"0","type":"EXEC","variable_list":[]}],"variable_list":[]},"type":"system"},{"critical":false,"description":"System action for stopping an application","name":"action_stop","runbook":{"description":"","main_task_local_reference":{"kind":"app_task","name":"6c01dc28_dag"},"name":"0cb172a7_runbook","task_definition_list":[{"attrs":{"edges":[],"type":""},"child_tasks_local_reference_list":[],"description":"","name":"6c01dc28_dag","retries":"0","target_any_local_reference":{"kind":"app_service","name":"Kubernetes_Minion"},"timeout_secs":"0","type":"DAG","variable_list":[]}],"variable_list":[]},"type":"system"}],"depends_on_list":[],"description":"","name":"Kubernetes_Minion","port_list":[],"singleton":false,"tier":"","variable_list":[]}],"substrate_definition_list":[{"action_list":[],"create_spec":{"availability_zone_reference":null,"backup_policy":null,"categories":"","cluster_reference":null,"name":"K8SC-@@{calm_array_index}@@-@@{calm_time}@@","resources":{"boot_config":{"boot_device":{"disk_address":{"adapter_type":"SCSI","device_index":0,"type":""},"type":""},"mac_address":"","type":""},"disk_list":[{"data_source_reference":{"kind":"image","name":"panlm-img-52.qcow2","type":"","uuid":"3d6737e5-b9e6-48d6-91ee-46ba538feb4e"},"device_properties":{"device_type":"DISK","disk_address":{"adapter_type":"SCSI","device_index":0,"type":""},"type":""},"disk_size_mib":0,"type":"","volume_group_reference":null}],"gpu_list":[],"guest_customization":{"cloud_init":{"meta_data":"","type":"","user_data":"#cloud-config\ndisable_root: False\nssh_enabled: True\nssh_pwauth: True\nusers:\n  - name: centos\n    ssh-authorized-keys:\n      - @@{INSTANCE_PUBLIC_KEY}@@\n    sudo: ['ALL=(ALL) NOPASSWD:ALL']"},"sysprep":null,"type":""},"guest_tools":null,"hardware_clock_timezone":"","memory_size_mib":4096,"nic_list":[{"ip_endpoint_list":[],"mac_address":"","network_function_chain_reference":null,"network_function_nic_type":"INGRESS","nic_type":"NORMAL_NIC","subnet_reference":{"kind":"subnet","name":"","type":"","uuid":"9ed22b7d-7cd5-410c-aa79-a35f453a25a2"},"type":""}],"num_sockets":2,"num_vcpus_per_socket":1,"parent_reference":null,"power_state":"ON","serial_port_list":[],"type":""},"type":""},"description":"","editables":{"create_spec":{"name":true,"resources":{"disk_list":{},"guest_customization":true,"memory_size_mib":true,"nic_list":{},"num_sockets":true,"num_vcpus_per_socket":true,"serial_port_list":{}}},"readiness_probe":{"connection_port":true,"connection_type":true,"timeout_secs":true}},"name":"AHV_Centos_K8SC","os_type":"Linux","readiness_probe":{"address":"@@{platform.status.resources.nic_list[0].ip_endpoint_list[0].ip}@@","connection_port":22,"connection_type":"SSH","delay_secs":"0","disable_readiness_probe":false,"login_credential_local_reference":{"kind":"app_credential","name":"CENTOS"}},"type":"AHV_VM","variable_list":[]},{"action_list":[],"create_spec":{"availability_zone_reference":null,"backup_policy":null,"categories":"","cluster_reference":null,"name":"K8SM-@@{calm_array_index}@@-@@{calm_time}@@","resources":{"boot_config":{"boot_device":{"disk_address":{"adapter_type":"SCSI","device_index":0,"type":""},"type":""},"mac_address":"","type":""},"disk_list":[{"data_source_reference":{"kind":"image","name":"panlm-img-52.qcow2","type":"","uuid":"3d6737e5-b9e6-48d6-91ee-46ba538feb4e"},"device_properties":{"device_type":"DISK","disk_address":{"adapter_type":"SCSI","device_index":0,"type":""},"type":""},"disk_size_mib":0,"type":"","volume_group_reference":null}],"gpu_list":[],"guest_customization":{"cloud_init":{"meta_data":"","type":"","user_data":"#cloud-config\ndisable_root: False\nssh_enabled: True\nssh_pwauth: True\nusers:\n  - name: centos\n    ssh-authorized-keys:\n      - @@{INSTANCE_PUBLIC_KEY}@@\n    sudo: ['ALL=(ALL) NOPASSWD:ALL']"},"sysprep":null,"type":""},"guest_tools":null,"hardware_clock_timezone":"","memory_size_mib":4096,"nic_list":[{"ip_endpoint_list":[],"mac_address":"","network_function_chain_reference":null,"network_function_nic_type":"INGRESS","nic_type":"NORMAL_NIC","subnet_reference":{"kind":"subnet","name":"","type":"","uuid":"9ed22b7d-7cd5-410c-aa79-a35f453a25a2"},"type":""}],"num_sockets":4,"num_vcpus_per_socket":1,"parent_reference":null,"power_state":"ON","serial_port_list":[],"type":""},"type":""},"description":"","editables":{"create_spec":{"name":true,"resources":{"disk_list":{},"guest_customization":true,"memory_size_mib":true,"nic_list":{},"num_sockets":true,"num_vcpus_per_socket":true,"serial_port_list":{}}},"readiness_probe":{"connection_port":true,"connection_type":true,"timeout_secs":true}},"name":"AHV_Centos_K8SM","os_type":"Linux","readiness_probe":{"address":"@@{platform.status.resources.nic_list[0].ip_endpoint_list[0].ip}@@","connection_port":22,"connection_type":"SSH","delay_secs":"0","disable_readiness_probe":false,"login_credential_local_reference":{"kind":"app_credential","name":"CENTOS"}},"type":"AHV_VM","variable_list":[]}],"type":"USER"}},"status":{}}